{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private Support Points: Regression Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston, load_diabetes, fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import time\n",
    "\n",
    "from sp_utils import (\n",
    "    scale_01,\n",
    "    get_energy_sensitivity,\n",
    "    get_support_points,\n",
    "    energy,\n",
    "    sample_sp_exp_mech,\n",
    "    sample_sp_mmd_dp_bw,\n",
    "    mixture_model_likelihood,\n",
    "    sample_full_set_given_bandwidth)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "font = {'family' : 'normal',\n",
    "        'size'   : 14}\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "log_filename = '../output/regression_logs/run.log'\n",
    "if os.path.isfile(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename,\n",
    "                             filemode='a',\n",
    "                             format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                             datefmt='%H:%M:%S',\n",
    "                             level=logging.DEBUG)\n",
    "mpl_logger = logging.getLogger('matplotlib') \n",
    "mpl_logger.setLevel(logging.WARNING)\n",
    "_LOG = logging.getLogger('[perturbed]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support points accurately model toy multivariate data in 5D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nd(d, w=10, h=10, title=None):\n",
    "    graph = pd.plotting.scatter_matrix(pd.DataFrame(d), figsize=(w, h));\n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test support points on multivariate Gaussian data.\n",
    "if 0:\n",
    "    M = 500\n",
    "    D = 5\n",
    "    data = np.random.normal(0, 1, size=(M, D))\n",
    "    N = 100\n",
    "    MAX_ITER = 501 #300\n",
    "    LR = 5e-1 #1e1\n",
    "\n",
    "    y_opt, e_opt = get_support_points(data, N, MAX_ITER, LR, is_tf=True,\n",
    "                                      Y_INIT_OPTION='uniform', clip='data')\n",
    "\n",
    "    plot_nd(pd.DataFrame(data), 5, 5, 'data')\n",
    "    plot_nd(pd.DataFrame(y_opt), 5, 5, 'sp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test variance of SP vs variance of Diffusion Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = 500\n",
    "N = 50\n",
    "dim = 2\n",
    "MAX_ITER = 201\n",
    "LR = 5e-1\n",
    "ENERGY_POWER = 2\n",
    "ALPHA = 10\n",
    "\n",
    "num_sp_samples = 3\n",
    "\n",
    "# Make data.\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.5], [0.5, 1]]\n",
    "data = np.random.multivariate_normal(mean, cov, [M])\n",
    "data = scale_01(data)\n",
    "\n",
    "if 0:\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Given data, repeated support point optimizations are low variance.\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    plt.scatter(data[:, 0], data[:, 1], c='gray', s=64,\n",
    "                alpha=0.3, label='data')\n",
    "    for i in range(num_sp_samples):\n",
    "        y_opt, e_opt = get_support_points(data, N, MAX_ITER, LR, is_tf=True,\n",
    "                                          Y_INIT_OPTION='uniform', clip='data',\n",
    "                                          plot=False)\n",
    "        plt.scatter(y_opt[:, 0], y_opt[:, 1], s=32, label='sp_{}'.format(i))\n",
    "\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Given support points, repeated diffusions create more variance.\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    # Get private support points.\n",
    "    y_opt, e_opt = get_support_points(data, N, MAX_ITER, LR, is_tf=True,\n",
    "                                      Y_INIT_OPTION='uniform', clip='data',\n",
    "                                      plot=False)\n",
    "    energy_sensitivity = 2 * dim ** (1. / ENERGY_POWER) * (2 * N - 1) / N ** 2\n",
    "    (y_tildes_diffusion,\n",
    "     energies_diffusion,\n",
    "     energy_errors_diffusion) = sample_sp_exp_mech(e_opt, energy_sensitivity,\n",
    "                                                   data, y_opt, 'mh',\n",
    "                                                   step_size=0.01,\n",
    "                                                   num_y_tildes=num_sp_samples,\n",
    "                                                   alpha=ALPHA,\n",
    "                                                   diffusion_mean=True)\n",
    "\n",
    "    plt.scatter(data[:, 0], data[:, 1], c='gray', s=64,\n",
    "                alpha=0.3, label='data')\n",
    "    plt.scatter(y_opt[:, 0], y_opt[:, 1], s=32, c='limegreen',\n",
    "                label='sp')\n",
    "    for i in range(num_sp_samples):\n",
    "        y_priv = y_tildes_diffusion[i]\n",
    "        plt.scatter(y_priv[:, 0], y_priv[:, 1], s=32, label='~sp_{}'.format(i))\n",
    "    plt.title(('|data|={}, |supp|={}, eps={:.3f}, e_opt: {:.8f}').format(\n",
    "                   M, N, ALPHA, e_opt))\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_regression(data, data_heldout):\n",
    "    \"\"\"Computes regression accuracy, averaged over multiple runs.\n",
    "    \n",
    "    Each run computes accuracy with k-fold cross validation.\n",
    "    \n",
    "    Args:\n",
    "      data (array): Training data.\n",
    "      data_heldout (array): Testing/heldout data.\n",
    "    \n",
    "    Returns:\n",
    "      result (scalar): MSE value on test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def regress(X_train, X_test, Y_train, Y_test):\n",
    "        # Fits linear model given a train-test split.\n",
    "        # Returns MSE value of fitted model.\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(X_train, Y_train)\n",
    "\n",
    "        Y_pred = lm.predict(X_test)\n",
    "        mse = mean_squared_error(Y_test, Y_pred)\n",
    "        r2 = lm.score(X_test, Y_test)\n",
    "        \n",
    "        return mse\n",
    "    \n",
    "        \n",
    "    result = regress(data[:, :-1], data_heldout[:, :-1],\n",
    "                     data[:, -1], data_heldout[:, -1])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for running experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Result = namedtuple('Result', 'dataset_name size alpha mse std tag')\n",
    "\n",
    "\n",
    "def visualize_data(data, title=None):\n",
    "    # Visualize data with pairs plot.\n",
    "    if len(data) >= 1000:\n",
    "        _d = data[np.random.choice(len(data), 500)]             \n",
    "        plot_nd(_d, 10, 10, title)\n",
    "    else:\n",
    "        plot_nd(data, 10, 10, title)\n",
    "\n",
    "\n",
    "def round_binary_cols(data, binary_cols):\n",
    "    \"\"\"Rounds binary cols of a NumPy array.\"\"\"\n",
    "    for col in binary_cols:\n",
    "        col_data = data[:, col]\n",
    "        col_data = np.clip(col_data, 0, 1)\n",
    "        col_data = np.round(col_data)\n",
    "        data[:, col] = col_data\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_accuracy_mean_std_on_heldout(candidate_sets, heldout, data,\n",
    "                                     dataset_name, N, alpha=None, tag=None):\n",
    "    \"\"\"Computes regression accuracy results for a multiset of candidate points.\n",
    "    \n",
    "    Multiset can be random subsets of data, can be sets of support points, or\n",
    "    can be sets of privatized support points.\n",
    "\n",
    "    Args:\n",
    "        candidate_sets (array): Multiset of candidate points.\n",
    "        heldout (array): Array of test data.\n",
    "        data (array): Data for plotting.\n",
    "        dataset_name (string): Name of data set.\n",
    "        N (int): Number of support points.\n",
    "        alpha (float): Privacy budget.\n",
    "        tag (string): Tag for Result object.\n",
    "\n",
    "    Returns:\n",
    "        result_priv (Result): Named tuple of result for private support points.\n",
    "    \"\"\"\n",
    "    # Collect accuracy values for each sample.\n",
    "    accuracies = []\n",
    "    for candidate_set in candidate_sets:\n",
    "        accuracy = test_regression(candidate_set, heldout)\n",
    "        accuracies.append(accuracy)\n",
    "    _LOG.info('accuracies_min_max = {}, {}'.format(min(accuracies),\n",
    "                                                   max(accuracies)))\n",
    "    _LOG.info('accuracies_std = {}'.format(np.std(accuracies)))\n",
    "\n",
    "    # Mean, Std of accuracies across samples.\n",
    "    accuracy_mean = np.round(np.mean(accuracies), 4)\n",
    "    accuracy_std = np.round(np.std(accuracies), 4)\n",
    "\n",
    "    # Store results of sample runs for this (N, alpha) combination.\n",
    "    result = Result(dataset_name, N, alpha, accuracy_mean, accuracy_std, tag)\n",
    "\n",
    "    # Visualize data versus sample of private support points.\n",
    "    #visualize_data(data, 'data')\n",
    "    #visualize_data(candidate_sets[-1], 'N={}, alpha={}'.format(N, alpha))\n",
    "\n",
    "    return result\n",
    "                \n",
    "\n",
    "def resample_from_histdd(H, edge_sets, n=100, plot=False):\n",
    "    \"\"\"Resamples data set from histogram, uniformly over bins.\n",
    "    \n",
    "    Args:\n",
    "        H (array): Arrays of counts per bin.\n",
    "        edge_sets (array): Arrays of edge boundaries per dim.\n",
    "        n (int): Number of points to sample.\n",
    "\n",
    "    Returns:\n",
    "        resampled (array): Newly sampled data.\n",
    "    \"\"\"\n",
    "    bin_widths = [np.diff(edges)[0] for edges in edge_sets]\n",
    "    midpoints = [edges[:-1] + np.diff(edges) / 2 for edges in edge_sets]\n",
    "\n",
    "    # Compute CDF of counts, then normalize.\n",
    "    cdf = np.cumsum(H.ravel())\n",
    "    cdf = cdf / cdf[-1]\n",
    "    \n",
    "    # Sample uniform, associate to CDF values.\n",
    "    values = np.random.rand(n)\n",
    "    value_bins = np.searchsorted(cdf, values)\n",
    "    \n",
    "    # Fetch associated indices from original grid.\n",
    "    unraveled_shape = [len(r) for r in midpoints]\n",
    "    hist_indices = np.array(np.unravel_index(value_bins, unraveled_shape))\n",
    "    \n",
    "    # Sample uniformly on bin.\n",
    "    random_from_cdf = []\n",
    "    num_dims = len(hist_indices)\n",
    "    for i in range(num_dims):\n",
    "        bin_width_i = bin_widths[i]\n",
    "        mids_i = midpoints[i][hist_indices[i]]\n",
    "        vals_i = mids_i + np.random.uniform(low=-bin_width_i / 2,\n",
    "                                            high=bin_width_i / 2,\n",
    "                                            size=mids_i.shape)\n",
    "        random_from_cdf.append(vals_i)\n",
    "    resampled = np.array(random_from_cdf).T\n",
    "    #random_from_cdf = np.array([midpoints[i][hist_indices[i]] for i in range(num_dims)])\n",
    "    \n",
    "    # Visualize data.\n",
    "    if plot:\n",
    "        visualize_data(resampled, 'resampled')\n",
    "        \n",
    "    return resampled\n",
    "    \n",
    "    \n",
    "def run_experiments(dataset_name, data_orig, num_supp, alphas,\n",
    "                    max_iter, lr, num_sp_samples, step_size, method='mh'):\n",
    "    \"\"\"Runs panel of experiments for different number of support points\n",
    "    and different alpha settings.\n",
    "    \n",
    "    Args:\n",
    "      dataset_name: String name.\n",
    "      data_orig: NumPy array of data. NOTE: Target var must be last column [!].\n",
    "      num_supp: List of support point set sizes.\n",
    "      alphas: List of alphas.\n",
    "      max_iter: Int, number of steps in support point optimization.\n",
    "      lr: Float, learning rate of support point optimization.\n",
    "      num_sp_samples: Int, number of Support Point sets over which \n",
    "        to average regression performance.\n",
    "      step_size: Float, step size of support point sampling.\n",
    "      method: String, name of private-sampling method. ['diffusion', 'mh']\n",
    "      \n",
    "    Returns:\n",
    "      results: List of Result objects.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Fetch, scale, and shuffle data.\n",
    "    data_scaled = data_orig\n",
    "    assert np.min(data_scaled) >= 0 and np.max(data_scaled) <= 1, 'Scaling incorrect.'\n",
    "    np.random.shuffle(data_scaled)\n",
    "\n",
    "    # Create folds for cross validation.\n",
    "    kf = KFold(n_splits=2)\n",
    "\n",
    "    # Do an experiment run for each train-test split.\n",
    "    for train_index, test_index in kf.split(data_scaled):\n",
    "        _LOG.info('Starting new data split.')\n",
    "        \n",
    "        if max(max(train_index), max(test_index)) >= len(data_scaled):\n",
    "            pdb.set_trace()\n",
    "        data = data_scaled[train_index]\n",
    "        data_heldout = data_scaled[test_index]\n",
    "    \n",
    "        visualize_data(data)\n",
    "    \n",
    "        # --------------------------------------\n",
    "        # Test regression on FULL TRAINING data.\n",
    "        # --------------------------------------\n",
    "        result_training_data = get_accuracy_mean_std_on_heldout(\n",
    "            [data], data_heldout,\n",
    "            data, dataset_name, len(data), \n",
    "            alpha=None, tag='full_training')\n",
    "        results.append(result_training_data)\n",
    "        _LOG.info(result_training_data)\n",
    "\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # Test regression on PERTURBED HISTOGRAM data.\n",
    "        # --------------------------------------------\n",
    "\n",
    "        for alpha in alphas:\n",
    "            n, d = data.shape\n",
    "            num_bins = (n * alpha / 10) ** ((2 * d) / (2 + d))\n",
    "            num_bins_per_dim = int(np.round(num_bins ** (1 / d)))  # https://arxiv.org/pdf/1504.05998.pdf\n",
    "\n",
    "            # Get true histogram, and perturb with Laplace noise.\n",
    "            H, edges = np.histogramdd(data, bins=num_bins_per_dim)\n",
    "\n",
    "            # Perturb histogram counts with Laplace noise.\n",
    "            H_perturbed = H + np.random.laplace(loc=0, scale=1/alpha, size=H.shape)\n",
    "\n",
    "            # Resample from perturbed histogram, using uniform sampling per bin.\n",
    "            perturbed_resample = resample_from_histdd(H_perturbed, edges, n=n)\n",
    "\n",
    "            # Evaluate regression performance on heldout data.\n",
    "            result_perturbed_resample = get_accuracy_mean_std_on_heldout(\n",
    "                [perturbed_resample], data_heldout,\n",
    "                perturbed_resample, dataset_name, len(perturbed_resample), \n",
    "                alpha=alpha, tag='perturbed_resample')\n",
    "            results.append(result_perturbed_resample)\n",
    "            _LOG.info(result_perturbed_resample)\n",
    "\n",
    "\n",
    "        # -------------------------------------------\n",
    "\n",
    "        for N in num_supp:\n",
    "\n",
    "            # -------------------------------------------\n",
    "            # Test regression on RANDOM SUBSETS (size N).\n",
    "            # -------------------------------------------\n",
    "            random_subsets_data = [\n",
    "                data[np.random.choice(len(data), N)]\n",
    "                for i in range(num_sp_samples)]\n",
    "            result_random_subsets_data = get_accuracy_mean_std_on_heldout(\n",
    "                random_subsets_data, data_heldout,\n",
    "                data, dataset_name, N, alpha=None, tag='random_subset')\n",
    "            results.append(result_random_subsets_data)\n",
    "            _LOG.info(result_random_subsets_data)\n",
    "\n",
    "\n",
    "            # ------------------------------------------------------\n",
    "            # Test regression on REPEATED SP optimizations (size N).\n",
    "            # ------------------------------------------------------\n",
    "            sp_sets = []\n",
    "            for i in range(num_sp_samples):\n",
    "                y_opt, e_opt = get_support_points(data, N, max_iter, lr, is_tf=True,\n",
    "                                                  Y_INIT_OPTION='uniform', clip='data',\n",
    "                                                  plot=False)\n",
    "                sp_sets.append(y_opt)\n",
    "            result_sp_sets = get_accuracy_mean_std_on_heldout(\n",
    "                sp_sets, data_heldout, data,\n",
    "                dataset_name, N, alpha=None, tag='support_points')\n",
    "            results.append(result_sp_sets)\n",
    "            _LOG.info(result_sp_sets)\n",
    "\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Test regression on PRIVATE SUPPORT POINTS (size N).\n",
    "            # ---------------------------------------------------\n",
    "\n",
    "            # For each alpha, compute private support points and test regression on them.\n",
    "            for alpha in alphas:\n",
    "                print(('\\n\\n---------------------------'\n",
    "                       '\\nRunning for N={}, alpha={}').format(N, alpha))\n",
    "                _LOG.info('Running for N={}, alpha={}'.format(N, alpha))\n",
    "\n",
    "                # TODO: TESTING FIXED STEP SIZES BASED ON ALPHA.\n",
    "                if alpha <= 1:\n",
    "                    step_size = 0.05\n",
    "                elif alpha <= 10:\n",
    "                    step_size = 0.01\n",
    "                else:\n",
    "                    step_size = 0.005\n",
    "\n",
    "\n",
    "                # -------------------------------   \n",
    "                # Compute private support points.\n",
    "                # -------------------------------   \n",
    "\n",
    "                energy_sensitivity = get_energy_sensitivity(data, N, alpha)\n",
    "                print(('Exp(2 * U / alpha) = Exp(2 * {:.4f} / {:.2f}) '\n",
    "                       '= Exp({:.3f})').format(energy_sensitivity, alpha, \n",
    "                                               2. * energy_sensitivity / alpha))\n",
    "                _LOG.info(('Exp(2 * U / alpha) = Exp(2 * {:.4f} / {:.2f}) '\n",
    "                           '= Exp({:.3f})').format(energy_sensitivity, alpha, \n",
    "                                                   2. * energy_sensitivity / alpha))\n",
    "\n",
    "                # For this (N, alpha) combo, run sampler many times.\n",
    "                # Resulting accuracy values will be averaged.\n",
    "                (y_tildes,\n",
    "                 energies,\n",
    "                 energy_errors) = sample_sp_exp_mech(e_opt, energy_sensitivity,\n",
    "                                                     data, y_opt, method,\n",
    "                                                     step_size=step_size,\n",
    "                                                     num_y_tildes=num_sp_samples,\n",
    "                                                     alpha=alpha,\n",
    "                                                     diffusion_mean=True)\n",
    "\n",
    "                private_sps = y_tildes\n",
    "\n",
    "\n",
    "                # ------------------------------------------   \n",
    "                # Test regression on private support points.\n",
    "                # ------------------------------------------\n",
    "\n",
    "                result = get_accuracy_mean_std_on_heldout(private_sps,\n",
    "                                                          data_heldout,\n",
    "                                                          data,\n",
    "                                                          dataset_name, N, alpha=alpha,\n",
    "                                                          tag='private_support_points')\n",
    "                results.append(result)\n",
    "                _LOG.info(result)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    return results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_filter_tag(rows, query, dataset, tag):\n",
    "    \"\"\"Gets results. Note: As saved, ordered by increasing N.\"\"\"\n",
    "    r = [d[query] for d in rows if\n",
    "         d['dataset_name'] == dataset and\n",
    "         d['tag'] == tag]\n",
    "    return r\n",
    "\n",
    "def dict_filter_alpha(rows, query, dataset, alpha):\n",
    "    \"\"\"Gets results. Note: As saved, ordered by increasing N.\"\"\"\n",
    "    r = [d[query] for d in rows if\n",
    "         d['dataset_name'] == dataset and\n",
    "         d['tag'] == 'private_support_points' and\n",
    "         d['alpha'] == alpha]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_results(results, dataset_name, alphas):\n",
    "    \"\"\"Plots regression results.\n",
    "    \n",
    "    Args:\n",
    "      results (list): List of Result objects.\n",
    "      dataset_name (string): Name of dataset, e.g. 'boston'.\n",
    "      alphas (list): List of alphas used in experiment panel.\n",
    "    \n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    \n",
    "    sizes = np.unique([d.size for d in results if d.tag == 'random_subset'])\n",
    "    x = np.linspace(0.2, 1, 5)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15,8))\n",
    "    fig.suptitle(dataset_name)\n",
    "    \n",
    "    # -------------------------------------\n",
    "    # Make boxplot for fixed size elements.\n",
    "    # -------------------------------------\n",
    "    \n",
    "    _full_training = [d.mse for d in results if d.tag == 'full_training']\n",
    "    _alpha_results = []\n",
    "    for a in alphas:\n",
    "        _pert_a = [\n",
    "            d.mse for d in results if\n",
    "            d.tag == 'perturbed_resample' and\n",
    "            d.alpha == a]\n",
    "        _alpha_results.append(_pert_a)\n",
    "        \n",
    "    ax1.boxplot(np.column_stack([_full_training] + _alpha_results),\n",
    "                labels=['train'] + ['pert:{}'.format(a) for a in alphas])\n",
    "\n",
    "    \n",
    "    \"\"\"    \n",
    "    _pert100 = [\n",
    "        d['mse'] for d in results if\n",
    "        d['tag'] == 'perturbed_resample' and\n",
    "        d['alpha'] == 100]\n",
    "    _pert10 = [\n",
    "        d['mse'] for d in results if\n",
    "        d['tag'] == 'perturbed_resample' and\n",
    "        d['alpha'] == 10]\n",
    "    _pert1 = [\n",
    "        d['mse'] for d in results if\n",
    "        d['tag'] == 'perturbed_resample' and\n",
    "        d['alpha'] == 1]\n",
    "    ax1.boxplot(np.column_stack((_full_training, _pert100, _pert10, _pert1)),\n",
    "                labels=['train', r'pert:100', r'pert:10', r'pert:1'])\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # ------------------------------------\n",
    "    # Then make line plot with error bars.\n",
    "    # ------------------------------------\n",
    "    \n",
    "    # Random subset.\n",
    "    mses = []\n",
    "    stds = []\n",
    "    for size in sizes:\n",
    "        res = [\n",
    "            d.mse for d in results if\n",
    "            d.size == size and\n",
    "            d.tag == 'random_subset']\n",
    "        mses.append(np.mean(res))\n",
    "        stds.append(np.std(res))\n",
    "   \n",
    "    ax2.errorbar(x, mses, yerr=stds, label='random_subset')\n",
    "\n",
    "    # Support points.\n",
    "    mses = []\n",
    "    stds = []\n",
    "    for size in sizes:\n",
    "        res = [\n",
    "            d.mse for d in results if\n",
    "            d.size == size and\n",
    "            d.tag == 'support_points']\n",
    "        mses.append(np.mean(res))\n",
    "        stds.append(np.std(res))\n",
    "    \n",
    "    ax2.errorbar(x + 0.01, mses, yerr=stds, label='support_points')\n",
    "    \n",
    "    # Private support points.\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        mses = []\n",
    "        stds = []\n",
    "        for size in sizes:\n",
    "            res = [d.mse for d in results if\n",
    "                   d.size == size and\n",
    "                   d.tag == 'private_support_points' and\n",
    "                   d.alpha == alpha]\n",
    "            mses.append(np.mean(res))\n",
    "            stds.append(np.std(res))\n",
    "\n",
    "        ax2.errorbar(x + 0.02 + i * 0.01,\n",
    "                     mses,\n",
    "                     yerr=stds,\n",
    "                     label=r'sp, $\\alpha={}$'.format(alpha))\n",
    "\n",
    "    # ------------------------------------    \n",
    "    \n",
    "    ax1.set_xlabel('Data used for fitting')    \n",
    "    ax2.set_xlabel('Number of Points, Fraction of Whole')\n",
    "    ax1.set_ylabel('Mean Squared Error')\n",
    "\n",
    "    ax2.legend()\n",
    "    plt.savefig('../output/regression_global_results_{}'.format(dataset_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_boston = 1\n",
    "run_diabetes = 1\n",
    "run_california = 1\n",
    "\n",
    "alphas = [1000, 100, 10, 1]\n",
    "num_sp_samples = 10\n",
    "num_cv_splits = 5\n",
    "\n",
    "global_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOSTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get Boston data.\n",
    "dataset_name = 'boston'\n",
    "dataset = load_boston()\n",
    "d = pd.DataFrame(dataset.data)\n",
    "d.columns = dataset.feature_names\n",
    "d['MEDV'] = dataset.target\n",
    "\n",
    "data = np.concatenate((dataset.data, dataset.target.reshape(-1, 1)), axis=1)\n",
    "data = scale_01(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Optional subsetting for troubleshooting.\n",
    "#data = data[np.random.choice(len(data), 200, replace=False)]\n",
    "#data = data[:, [1, 2, 3]]\n",
    "#plot_nd(data, title='Data, scaled [0, 1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of parameters to test.\n",
    "num_supp = [int(len(data) / num_cv_splits * i / 10) for i in range(2, 11, 2)] \n",
    "max_iter = 301\n",
    "lr = 0.5\n",
    "step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run experiments and print results.\n",
    "if run_boston:\n",
    "    np.random.seed(123)\n",
    "    results = run_experiments(dataset_name, data, num_supp, alphas,\n",
    "                              max_iter, lr, num_sp_samples, step_size)\n",
    "    \n",
    "    global_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_boston:\n",
    "    plot_final_results(results, 'boston', alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIABETES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get diabetes data.\n",
    "dataset_name = 'diabetes'\n",
    "dataset = load_diabetes()\n",
    "d = pd.DataFrame(dataset.data)\n",
    "d.columns = dataset.feature_names\n",
    "d['TARGET'] = dataset.target\n",
    "\n",
    "data = np.concatenate((dataset.data, dataset.target.reshape(-1, 1)), axis=1)\n",
    "data = scale_01(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Optional subsetting for troubleshooting.\n",
    "#data = data[np.random.choice(len(data), 200, replace=False)]\n",
    "#data = data[:, [1, 2, 3]]\n",
    "#plot_nd(data, title='Data, scaled [0, 1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of parameters to test.\n",
    "num_supp = [int(len(data) / num_cv_splits * i / 10) for i in range(2, 11, 2)] \n",
    "max_iter = 301\n",
    "lr = 0.5\n",
    "step_size = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run experiments and print results.\n",
    "if run_diabetes:\n",
    "    np.random.seed(123)\n",
    "    results = run_experiments(dataset_name, data, num_supp, alphas,\n",
    "                              max_iter, lr, num_sp_samples, step_size)\n",
    "    \n",
    "    global_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_diabetes:\n",
    "    plot_final_results(results, 'diabetes', alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CALIFORNIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get diabetes data.\n",
    "dataset_name = 'california'\n",
    "dataset = fetch_california_housing()\n",
    "d = pd.DataFrame(dataset.data)\n",
    "d.columns = dataset.feature_names\n",
    "d['TARGET'] = dataset.target\n",
    "\n",
    "data = np.concatenate((dataset.data, dataset.target.reshape(-1, 1)), axis=1)\n",
    "data = scale_01(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Optional subsetting for troubleshooting.\n",
    "#data = data[np.random.choice(len(data), 200, replace=False)]\n",
    "#data = data[:, [1, 2, 3]]\n",
    "#plot_nd(data, title='Data, scaled [0, 1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of parameters to test.\n",
    "num_supp = [int(len(data) / num_cv_splits * i / 10) for i in range(2, 11, 2)] \n",
    "max_iter = 501\n",
    "lr = 1.\n",
    "step_size = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run experiments and print results.\n",
    "if run_california:\n",
    "    np.random.seed(123)\n",
    "    results = run_experiments(dataset_name, data, num_supp, alphas,\n",
    "                              max_iter, lr, num_sp_samples, step_size)\n",
    "\n",
    "    global_results.extend(results)\n",
    "    plot_final_results(results, 'california', alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_california:\n",
    "    plot_final_results(results, 'california', alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and plot global results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to file.\n",
    "import json\n",
    "gr_as_dict = [row._asdict() for row in global_results]\n",
    "with open('../output/regression_global_results.json', 'w') as outfile:\n",
    "    json.dump(gr_as_dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
