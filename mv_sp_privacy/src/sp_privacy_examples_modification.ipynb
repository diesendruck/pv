{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Private Support Points: Regression Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pdb\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston, load_diabetes, fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import time\n",
    "\n",
    "from sp_utils import (\n",
    "    scale_01,\n",
    "    get_energy_sensitivity,\n",
    "    get_support_points,\n",
    "    energy,\n",
    "    sample_sp_exp_mech,\n",
    "    #sample_sp_mmd_dp_bw,\n",
    "    mixture_model_likelihood,\n",
    "    sample_full_set_given_bandwidth)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "#font = {'family' : 'normal',\n",
    "#        'size'   : 14}\n",
    "#matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging.\n",
    "log_filename = '../output/regression_logs/run.log'\n",
    "if os.path.isfile(log_filename):\n",
    "    os.remove(log_filename)\n",
    "logging.basicConfig(filename=log_filename,\n",
    "                             filemode='a',\n",
    "                             format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                             datefmt='%H:%M:%S',\n",
    "                             level=logging.DEBUG)\n",
    "mpl_logger = logging.getLogger('matplotlib') \n",
    "mpl_logger.setLevel(logging.WARNING)\n",
    "_LOG = logging.getLogger('[perturbed]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support points accurately model toy multivariate data in 5D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nd(d, w=10, h=10, title=None):\n",
    "    graph = pd.plotting.scatter_matrix(pd.DataFrame(d), figsize=(w, h));\n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test support points on multivariate Gaussian data.\n",
    "if 0:\n",
    "    num_data = 500\n",
    "    DIM = 5\n",
    "    data = np.random.normal(0, 1, size=(num_data, DIM))\n",
    "    data = scale_01(data)\n",
    "    num_supp = 100\n",
    "    MAX_ITER = 501 #300\n",
    "    LR = 1e-1 #1e1\n",
    "    ENERGY_POWER = 1\n",
    "\n",
    "    y_opt, e_opt = get_support_points(data, num_supp, MAX_ITER, LR, is_tf=True,\n",
    "                                      power=ENERGY_POWER, y_init_option='uniform',\n",
    "                                      clip='bounds')\n",
    "\n",
    "    plot_nd(pd.DataFrame(data), 5, 5, 'data')\n",
    "    plot_nd(pd.DataFrame(y_opt), 5, 5, 'sp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test variance of Support Point Optimization vs variance of Metropolis Hastings Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_data = 500\n",
    "num_supp = 50\n",
    "DIM = 2\n",
    "MAX_ITER = 201\n",
    "LR = 5e-1\n",
    "ALPHA = 1000\n",
    "POWER = 1\n",
    "\n",
    "\n",
    "num_sp_samples = 3\n",
    "\n",
    "# Make data.\n",
    "mean = [0, 0]\n",
    "cov = [[1, 0.5], [0.5, 1]]\n",
    "data = np.random.multivariate_normal(mean, cov, [num_data])\n",
    "data = scale_01(data)\n",
    "energy_sensitivity = get_energy_sensitivity(data, num_supp, power=POWER)\n",
    "\n",
    "\n",
    "if 0:\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # Given data, repeated support point optimizations are low variance.\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    plt.scatter(data[:, 0], data[:, 1], c='gray', s=64,\n",
    "                alpha=0.3, label='data')\n",
    "    for i in range(num_sp_samples):\n",
    "        y_opt, e_opt = get_support_points(data, num_supp, MAX_ITER, LR, is_tf=True,\n",
    "                                          power=POWER, y_init_option='uniform',\n",
    "                                          clip='data', plot=False)\n",
    "        plt.scatter(y_opt[:, 0], y_opt[:, 1], s=32, label='sp_{}'.format(i))\n",
    "\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------\n",
    "    # Given support points, repeated samples from Exp.Mech. creates more variance.\n",
    "    # ---------------------------------------------------------------\n",
    "\n",
    "    # Get private support points.\n",
    "    y_opt, e_opt = get_support_points(data, num_supp, MAX_ITER, LR, is_tf=True,\n",
    "                                      power=POWER, y_init_option='uniform',\n",
    "                                      clip='data', plot=False)\n",
    "    (y_tildes,\n",
    "     energies,\n",
    "     energy_errors) = sample_sp_exp_mech(e_opt, energy_sensitivity, data, y_opt,\n",
    "                                         method='mh', num_y_tildes=num_sp_samples,\n",
    "                                         alpha=ALPHA,\n",
    "                                         save_dir='../output/regression_logs',\n",
    "                                         power=POWER,)\n",
    "\n",
    "    plt.scatter(data[:, 0], data[:, 1], c='gray', s=64,\n",
    "                alpha=0.3, label='data')\n",
    "    plt.scatter(y_opt[:, 0], y_opt[:, 1], s=32, c='limegreen',\n",
    "                label='sp')\n",
    "    for i in range(num_sp_samples):\n",
    "        y_priv = y_tildes[i]\n",
    "        plt.scatter(y_priv[:, 0], y_priv[:, 1], s=32, label='~sp_{}'.format(i))\n",
    "    plt.title(('|data|={}, |supp|={}, eps={:.3f}, e_opt: {:.8f}').format(\n",
    "                   num_data, num_supp, ALPHA, e_opt))\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_regression(data, data_heldout):\n",
    "    \"\"\"Computes regression accuracy, averaged over multiple runs.\n",
    "    \n",
    "    Each run computes accuracy with k-fold cross validation.\n",
    "    \n",
    "    Args:\n",
    "      data (array): Training data.\n",
    "      data_heldout (array): Testing/heldout data.\n",
    "    \n",
    "    Returns:\n",
    "      result (scalar): MSE value on test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def regress(X_train, X_test, Y_train, Y_test, polynomial=False):\n",
    "        if polynomial:\n",
    "            poly = PolynomialFeatures(degree=3)\n",
    "            X_train = poly.fit_transform(X_train)\n",
    "            X_test = poly.fit_transform(X_test)\n",
    "            \n",
    "        # Fits linear model given a train-test split.\n",
    "        # Returns MSE value of fitted model.\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(X_train, Y_train)\n",
    "\n",
    "        Y_pred = lm.predict(X_test)\n",
    "        mse = mean_squared_error(Y_test, Y_pred)\n",
    "        \n",
    "        return mse\n",
    "    \n",
    "    \"\"\"\n",
    "    def regress(X_train, X_test, Y_train, Y_test):\n",
    "        # Fits linear model given a train-test split.\n",
    "        # Returns MSE value of fitted model.\n",
    "        lm = LinearRegression()\n",
    "        lm.fit(X_train, Y_train)\n",
    "\n",
    "        Y_pred = lm.predict(X_test)\n",
    "        mse = mean_squared_error(Y_test, Y_pred)\n",
    "        r2 = lm.score(X_test, Y_test)\n",
    "        \n",
    "        return mse\n",
    "    \"\"\"\n",
    "        \n",
    "    result = regress(data[:, :-1], data_heldout[:, :-1],\n",
    "                     data[:, -1], data_heldout[:, -1], polynomial=False)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for running experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Result = namedtuple('Result', 'dataset_name size alpha mse std tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(data, title=None):\n",
    "    # Visualize data with pairs plot.\n",
    "    if len(data) >= 1000:\n",
    "        _d = data[np.random.choice(len(data), 500)]             \n",
    "        plot_nd(_d, 10, 10, title)\n",
    "    else:\n",
    "        plot_nd(data, 10, 10, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_mean_std_on_heldout(candidate_sets, heldout, data,\n",
    "                                     dataset_name, num_supp, alpha=None, tag=None):\n",
    "    \"\"\"Computes regression accuracy results for a multiset of candidate points.\n",
    "    \n",
    "    Multiset can be random subsets of data, can be sets of support points, or\n",
    "    can be sets of privatized support points.\n",
    "\n",
    "    Args:\n",
    "        candidate_sets (array): Multiset of candidate points.\n",
    "        heldout (array): Array of test data.\n",
    "        data (array): Data for plotting.\n",
    "        dataset_name (string): Name of data set.\n",
    "        num_supp (int): Number of support points.\n",
    "        alpha (float): Privacy budget.\n",
    "        tag (string): Tag for Result object.\n",
    "\n",
    "    Returns:\n",
    "        result_priv (Result): Named tuple of result for private support points.\n",
    "    \"\"\"\n",
    "    # Collect accuracy values for each sample.\n",
    "    accuracies = []\n",
    "    for candidate_set in candidate_sets:\n",
    "        accuracy = test_regression(candidate_set, heldout)\n",
    "        accuracies.append(accuracy)\n",
    "    _LOG.info('accuracies_min_max = {}, {}'.format(min(accuracies),\n",
    "                                                   max(accuracies)))\n",
    "    _LOG.info('accuracies_std = {}'.format(np.std(accuracies)))\n",
    "\n",
    "    # Mean, Std of accuracies across samples.\n",
    "    accuracy_mean = np.round(np.mean(accuracies), 4)\n",
    "    accuracy_std = np.round(np.std(accuracies), 4)\n",
    "\n",
    "    # Store results of sample runs for this (num_supp, alpha) combination.\n",
    "    result = Result(dataset_name, num_supp, alpha, accuracy_mean, accuracy_std, tag)\n",
    "\n",
    "    # Visualize data versus sample of private support points.\n",
    "    #visualize_data(data, 'data')\n",
    "    #visualize_data(candidate_sets[-1], 'num_supp={}, alpha={}'.format(num_supp, alpha))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_from_histdd(H, edge_sets, n=100, plot=False):\n",
    "    \"\"\"Resamples data set from histogram, uniformly over bins.\n",
    "    \n",
    "    Args:\n",
    "        H (array): Arrays of counts per bin.\n",
    "        edge_sets (array): Arrays of edge boundaries per dim.\n",
    "        n (int): Number of points to sample.\n",
    "\n",
    "    Returns:\n",
    "        resampled (array): Newly sampled data.\n",
    "    \"\"\"\n",
    "    bin_widths = [np.diff(edges)[0] for edges in edge_sets]\n",
    "    midpoints = [edges[:-1] + np.diff(edges) / 2 for edges in edge_sets]\n",
    "\n",
    "    # Compute CDF of counts, then normalize.\n",
    "    cdf = np.cumsum(H.ravel())\n",
    "    cdf = cdf / cdf[-1]\n",
    "    \n",
    "    # Sample uniform, associate to CDF values.\n",
    "    values = np.random.rand(n)\n",
    "    value_bins = np.searchsorted(cdf, values)\n",
    "    \n",
    "    # Fetch associated indices from original grid.\n",
    "    unraveled_shape = [len(r) for r in midpoints]\n",
    "    hist_indices = np.array(np.unravel_index(value_bins, unraveled_shape))\n",
    "    \n",
    "    # Sample uniformly on bin.\n",
    "    random_from_cdf = []\n",
    "    num_dims = len(hist_indices)\n",
    "    for i in range(num_dims):\n",
    "        bin_width_i = bin_widths[i]\n",
    "        mids_i = midpoints[i][hist_indices[i]]\n",
    "        vals_i = mids_i + np.random.uniform(low=-bin_width_i / 2,\n",
    "                                            high=bin_width_i / 2,\n",
    "                                            size=mids_i.shape)\n",
    "        random_from_cdf.append(vals_i)\n",
    "    resampled = np.array(random_from_cdf).T\n",
    "    #random_from_cdf = np.array([midpoints[i][hist_indices[i]] for i in range(num_dims)])\n",
    "    \n",
    "    # Visualize data.\n",
    "    if plot:\n",
    "        visualize_data(resampled, 'resampled')\n",
    "        \n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(dataset_name, data_orig, num_supp_list, alphas,\n",
    "                    max_iter, lr, num_sp_samples, method='mh',\n",
    "                    burnin=5000, num_cv_splits=None, power=None):\n",
    "    \"\"\"Runs panel of experiments for different number of support points\n",
    "    and different alpha settings.\n",
    "    \n",
    "    Args:\n",
    "      dataset_name: String name.\n",
    "      data_orig: NumPy array of data. NOTE: Target var must be last column [!].\n",
    "      num_supp_list: List of support point set sizes.\n",
    "      alphas: List of alphas.\n",
    "      max_iter: Int, number of steps in support point optimization.\n",
    "      lr: Float, learning rate of support point optimization.\n",
    "      num_sp_samples: Int, number of Support Point sets over which \n",
    "        to average regression performance.\n",
    "      method: String, name of private-sampling method. ['diffusion', 'mh']\n",
    "      burnin: Int, number of samples to burn in MH sampler.\n",
    "      num_cv_splits: Int, number of cross validation splits.\n",
    "      power: Int, power in energy metric. [1, 2]\n",
    "      \n",
    "    Returns:\n",
    "      results: List of Result objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    # Fetch, scale, and shuffle data.\n",
    "    data_scaled = data_orig\n",
    "    assert np.min(data_scaled) >= 0 and np.max(data_scaled) <= 1, 'Scaling incorrect.'\n",
    "    np.random.shuffle(data_scaled)\n",
    "\n",
    "    # Create folds for cross validation.\n",
    "    kf = KFold(n_splits=num_cv_splits)\n",
    "\n",
    "    # Do an experiment run for each train-test split.\n",
    "    for train_index, test_index in kf.split(data_scaled):\n",
    "        \n",
    "        print('Starting new data split.')\n",
    "        _LOG.info('Starting new data split.')\n",
    "        \n",
    "        if max(max(train_index), max(test_index)) >= len(data_scaled):\n",
    "            pdb.set_trace()\n",
    "        data = data_scaled[train_index]\n",
    "        data_heldout = data_scaled[test_index]\n",
    "    \n",
    "        visualize_data(data, title='data')\n",
    "    \n",
    "        # --------------------------------------\n",
    "        # Test regression on FULL TRAINING data.\n",
    "        # --------------------------------------\n",
    "        \n",
    "        result_training_data = get_accuracy_mean_std_on_heldout(\n",
    "            [data], data_heldout,\n",
    "            data, dataset_name, len(data), \n",
    "            alpha=None, tag='full_training')\n",
    "        results.append(result_training_data)\n",
    "        _LOG.info(result_training_data)\n",
    "\n",
    "\n",
    "        # --------------------------------------------\n",
    "        # Test regression on PERTURBED HISTOGRAM data.\n",
    "        # --------------------------------------------\n",
    "        \n",
    "        _LOG.info('Starting perturbed histograms.')\n",
    "\n",
    "        for alpha in alphas:\n",
    "            _N, _DIM = data.shape\n",
    "            num_bins = (_N * alpha / 10) ** ((2 * _DIM) / (2 + _DIM))\n",
    "            num_bins_per_dim = min(20, int(np.round(num_bins ** (1 / _DIM))))  # https://arxiv.org/pdf/1504.05998.pdf\n",
    "            try:\n",
    "                # Get true histogram, and perturb with Laplace noise.\n",
    "                H, edges = np.histogramdd(data, bins=num_bins_per_dim)\n",
    "            except:\n",
    "                print('[!] Perturbed histogram does not fit in memory. Skipping.')\n",
    "                _LOG.warning(('Skipping histogram for dataset={}, alpha={}. '\n",
    "                              'num_bins_per_dim ** dim, {} ** {} is too large.').format(\n",
    "                    dataset_name, alpha, num_bins_per_dim, _DIM))\n",
    "                break\n",
    "            \n",
    "            # Perturb histogram counts with Laplace noise.\n",
    "            H_perturbed = H + np.random.laplace(loc=0, scale=1/alpha, size=H.shape)\n",
    "\n",
    "            # Resample from perturbed histogram, using uniform sampling per bin.\n",
    "            perturbed_hist = resample_from_histdd(H_perturbed, edges, n=_N)\n",
    "\n",
    "            \n",
    "            # Evaluate regression performance on heldout data.\n",
    "            result_perturbed_hist = get_accuracy_mean_std_on_heldout(\n",
    "                [perturbed_hist], data_heldout, perturbed_hist, dataset_name,\n",
    "                len(perturbed_hist),alpha=alpha, tag='perturbed_hist')\n",
    "            results.append(result_perturbed_hist)\n",
    "            _LOG.info(result_perturbed_hist)\n",
    "\n",
    "        # -------------------------------------------\n",
    "\n",
    "        \n",
    "        for num_supp in num_supp_list:\n",
    "\n",
    "            # --------------------------------------------------\n",
    "            # Test regression on RANDOM SUBSETS (size num_supp).\n",
    "            # --------------------------------------------------\n",
    "            \n",
    "            random_subsets_data = [\n",
    "                data[np.random.choice(len(data), num_supp)]\n",
    "                for i in range(num_sp_samples)]\n",
    "            result_random_subsets_data = get_accuracy_mean_std_on_heldout(\n",
    "                random_subsets_data, data_heldout,\n",
    "                data, dataset_name, num_supp, alpha=None, tag='random_subset')\n",
    "            results.append(result_random_subsets_data)\n",
    "            _LOG.info(result_random_subsets_data)\n",
    "\n",
    "\n",
    "            # -------------------------------------------------------------\n",
    "            # Test regression on REPEATED SP optimizations (size num_supp).\n",
    "            # -------------------------------------------------------------\n",
    "\n",
    "            _LOG.info('Starting repeated SP optimizations, num_supp={}.'.format(num_supp))\n",
    "            \n",
    "            sp_sets = []\n",
    "            for i in range(num_sp_samples):\n",
    "            #for i in range(1): # TODO: remove this line.\n",
    "                y_opt, e_opt = get_support_points(data, num_supp, max_iter, lr,\n",
    "                                                  is_tf=True,\n",
    "                                                  power=power,\n",
    "                                                  y_init_option='uniform',\n",
    "                                                  clip='data', plot=False)\n",
    "                sp_sets.append(y_opt)\n",
    "            \n",
    "            # Show an example of support points.\n",
    "            visualize_data(y_opt, title='SP, num_supp={}'.format(num_supp))\n",
    "            \n",
    "            result_sp_sets = get_accuracy_mean_std_on_heldout(\n",
    "                sp_sets, data_heldout, data,\n",
    "                dataset_name, num_supp, alpha=None, tag='support_points')\n",
    "            results.append(result_sp_sets)\n",
    "            _LOG.info(result_sp_sets)\n",
    "            \n",
    "            e_rand = np.mean([energy(random_subsets_data[j], data, power=power)[0] for j in range(len(random_subsets_data))])\n",
    "            e_supp = np.mean([energy(sp_sets[j], data, power=power)[0] for j in range(len(random_subsets_data))])\n",
    "            print('num_supp={}, e_rand={:.8f}, e_supp={:.8f}'.format(num_supp, e_rand, e_supp))\n",
    "\n",
    "\n",
    "            # ---------------------------------------------------\n",
    "            # Test regression on PRIVATE SUPPORT POINTS.\n",
    "            # ---------------------------------------------------\n",
    "            \n",
    "            # Test various sizes of true support points (num_supp).\n",
    "            optimal_support_points, e_opt = get_support_points(\n",
    "                data, num_supp, max_iter, lr, is_tf=True, y_init_option='uniform',\n",
    "                clip='data', plot=False, power=power)\n",
    "\n",
    "            # For each alpha, compute private support points and test regression on them.\n",
    "            for alpha in alphas:\n",
    "                #if alpha > 10000:\n",
    "                #    print('For alpha={}, using burnin=20000'.format(alpha))\n",
    "                #    burnin = 20000\n",
    "                #else:\n",
    "                #    print('For alpha={}, using burnin=5000'.format(alpha))\n",
    "                #    burnin = 5000\n",
    "\n",
    "                print('Starting private SP, num_supp={}, alpha={}'.format(num_supp, alpha))\n",
    "                _LOG.info('Starting private SP, num_supp={}, alpha={}'.format(num_supp, alpha))\n",
    "\n",
    "\n",
    "                # -----------------------------\n",
    "                # Compute private SPs directly.\n",
    "                # -----------------------------\n",
    "\n",
    "                energy_sensitivity = get_energy_sensitivity(data, num_supp, power=power)\n",
    "                print(('Exp(2 * U / alpha) = Exp(2 * {:.4f} / {:.2f}) '\n",
    "                       '= Exp({:.8f})').format(energy_sensitivity, alpha, \n",
    "                                               2. * energy_sensitivity / alpha))\n",
    "                _LOG.info(('Exp(2 * U / alpha) = Exp(2 * {:.4f} / {:.2f}) '\n",
    "                           '= Exp({:.8f})').format(energy_sensitivity, alpha, \n",
    "                                                   2. * energy_sensitivity / alpha))\n",
    "\n",
    "                # Compute SP directly.\n",
    "                (y_tildes,\n",
    "                 energies,\n",
    "                 energy_errors) = sample_sp_exp_mech(energy_sensitivity, data, num_supp,\n",
    "                                                     method='mh',\n",
    "                                                     num_y_tildes=num_sp_samples,\n",
    "                                                     alpha=alpha,\n",
    "                                                     burnin=burnin,\n",
    "                                                     plot=False,  # Set plot=True for troubleshooting.\n",
    "                                                     save_dir='../output/regression_logs',\n",
    "                                                     power=power,\n",
    "                                                     optimal_support_points=optimal_support_points)\n",
    "                private_sps_direct = y_tildes\n",
    "                \n",
    "                # Test regression and store results.\n",
    "                result_direct = get_accuracy_mean_std_on_heldout(\n",
    "                    private_sps_direct, data_heldout, data, dataset_name, num_supp, alpha=alpha,\n",
    "                    tag='private_support_points_direct')\n",
    "                results.append(result_direct)                \n",
    "\n",
    "                \n",
    "                # -------------------------------\n",
    "                # Compute private SPs indirectly.\n",
    "                # -------------------------------\n",
    "                \n",
    "                # Using variably-sized natural SP, compute fixed-size private SP\".\n",
    "                fixed_private_size = 20\n",
    "                (y_tildes,\n",
    "                 energies,\n",
    "                 energy_errors) = sample_sp_exp_mech(energy_sensitivity, optimal_support_points, fixed_private_size,\n",
    "                                                     method='mh',\n",
    "                                                     num_y_tildes=num_sp_samples,\n",
    "                                                     alpha=alpha,\n",
    "                                                     burnin=burnin,\n",
    "                                                     plot=False,  # Set plot=True for troubleshooting.\n",
    "                                                     save_dir='../output/regression_logs',\n",
    "                                                     power=power,\n",
    "                                                     optimal_support_points=optimal_support_points)\n",
    "                private_sps_indirect = y_tildes\n",
    "\n",
    "                # Show examples of private support points.\n",
    "                visualize_data(private_sps_direct[-1],\n",
    "                               title='Direct ~SP, num_supp={}, alpha={}'.format(num_supp, alpha))\n",
    "                visualize_data(private_sps_indirect[-1],\n",
    "                               title='Indirect ~SP, num_supp={}, alpha={}'.format(num_supp, alpha))\n",
    "\n",
    "                # Test regression and store results.\n",
    "                result_indirect = get_accuracy_mean_std_on_heldout(\n",
    "                    private_sps_indirect, data_heldout, data, dataset_name, num_supp, alpha=alpha,\n",
    "                    tag='private_support_points_indirect')\n",
    "                results.append(result_indirect)\n",
    "\n",
    "            \n",
    "    return results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_filter_tag(rows, query, dataset, tag):\n",
    "    \"\"\"Gets results. Note: As saved, ordered by increasing N.\"\"\"\n",
    "    r = [d[query] for d in rows if\n",
    "         d['dataset_name'] == dataset and\n",
    "         d['tag'] == tag]\n",
    "    return r\n",
    "\n",
    "def dict_filter_alpha(rows, query, dataset, alpha):\n",
    "    \"\"\"Gets results. Note: As saved, ordered by increasing N.\"\"\"\n",
    "    r = [d[query] for d in rows if\n",
    "         d['dataset_name'] == dataset and\n",
    "         d['tag'] == 'private_support_points' and\n",
    "         d['alpha'] == alpha]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_results(results, dataset_name, x_ticks, alphas):\n",
    "    \"\"\"Plots regression results.\n",
    "    \n",
    "    Args:\n",
    "      results (list): List of Result objects.\n",
    "      dataset_name (string): Name of dataset, e.g. 'boston'.\n",
    "      x_ticks (list/np array): Percentage sizes of sp sets.\n",
    "      alphas (list): List of alphas used in experiment panel.\n",
    "    \n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    \n",
    "    #sizes = np.unique([d.size for d in results if d.tag == 'random_subset'])\n",
    "    sizes = np.unique([d.size for d in results if d.tag == 'private_support_points_direct'])\n",
    "    x = np.array(x_ticks)\n",
    "    x_jitter = (max(x) - min(x)) / 30.\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(15, 8))\n",
    "    fig.suptitle(dataset_name)\n",
    "    \n",
    "    # -------------------------------------\n",
    "    # Make boxplot for fixed size elements.\n",
    "    # -------------------------------------\n",
    "    \n",
    "    _full_training = [d.mse for d in results if d.tag == 'full_training']\n",
    "    \n",
    "    # Perturbed histogram results.\n",
    "    _hist_results = []\n",
    "    for a in alphas:\n",
    "        _hist_a = [\n",
    "            d.mse for d in results if\n",
    "            d.tag == 'perturbed_hist' and\n",
    "            d.alpha == a]\n",
    "        _hist_results.append(_hist_a)\n",
    "\n",
    "    # Fetch non-empty results for boxplot.\n",
    "    if len(_hist_results[0]) == 0:\n",
    "        boxplot_contents = _full_training\n",
    "        boxplot_labels = ['train']\n",
    "    else:\n",
    "        boxplot_contents = np.column_stack([_full_training] + _hist_results)\n",
    "        boxplot_labels = ['train'] + ['hist:{}'.format(a) for a in alphas]\n",
    "\n",
    "    ax1.boxplot(boxplot_contents, labels=boxplot_labels)\n",
    "    ax1.set_ylim([0, 1])\n",
    "\n",
    "    # ------------------------------------\n",
    "    # Then make line plot with error bars.\n",
    "    # ------------------------------------\n",
    "    \n",
    "    # Random subset.\n",
    "    mses = []\n",
    "    stds = []\n",
    "    for size in sizes:\n",
    "        res = [\n",
    "            d.mse for d in results if\n",
    "            d.size == size and\n",
    "            d.tag == 'random_subset']\n",
    "        mses.append(np.mean(res))\n",
    "        stds.append(np.std(res))\n",
    "   \n",
    "    ax2.errorbar(x, mses, yerr=stds, label='random_subset')\n",
    "\n",
    "    # Support points.\n",
    "    mses = []\n",
    "    stds = []\n",
    "    for size in sizes:\n",
    "        res = [\n",
    "            d.mse for d in results if\n",
    "            d.size == size and\n",
    "            d.tag == 'support_points']\n",
    "        mses.append(np.mean(res))\n",
    "        stds.append(np.std(res))\n",
    "    \n",
    "    ax2.errorbar(x + x_jitter, mses, yerr=stds, label='support_points')\n",
    "        \n",
    "    # Private support points direct.\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        mses = []\n",
    "        stds = []\n",
    "        for size in sizes:\n",
    "            res = [d.mse for d in results if\n",
    "                   d.size == size and\n",
    "                   d.tag == 'private_support_points_direct' and\n",
    "                   d.alpha == alpha]\n",
    "            \n",
    "            mses.append(np.mean(res))\n",
    "            stds.append(np.std(res))\n",
    "\n",
    "        ax2.errorbar(x + 2 * x_jitter + i * x_jitter,\n",
    "                     mses,\n",
    "                     yerr=stds,\n",
    "                     label=r'sp-dir, $\\alpha={}$'.format(alpha))\n",
    "        \n",
    "    # Private support points indirect.\n",
    "    for i, alpha in enumerate(alphas):\n",
    "        mses = []\n",
    "        stds = []\n",
    "        for size in sizes:\n",
    "            res = [d.mse for d in results if\n",
    "                   d.size == size and\n",
    "                   d.tag == 'private_support_points_indirect' and\n",
    "                   d.alpha == alpha]\n",
    "            \n",
    "            mses.append(np.mean(res))\n",
    "            stds.append(np.std(res))\n",
    "\n",
    "        ax2.errorbar(x + 2 * x_jitter + i * x_jitter,\n",
    "                     mses,\n",
    "                     yerr=stds,\n",
    "                     label=r'sp-indir, $\\alpha={}$'.format(alpha))\n",
    "\n",
    "    # ------------------------------------    \n",
    "    \n",
    "    ax1.set_xlabel('Data used for fitting')    \n",
    "    ax2.set_xlabel('Number of Points, Fraction of Whole')\n",
    "    ax1.set_ylabel('Mean Squared Error')\n",
    "\n",
    "    ax2.legend()\n",
    "    plt.savefig('../output/regression_logs/global_results_{}_modification'.format(dataset_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To shorten run: alphas, sp_samples, cv_splits, burnin, thinning, for i in range(num_samples), subsetting count, subsetting dims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_boston = 1\n",
    "run_diabetes = 0\n",
    "run_california = 0\n",
    "\n",
    "alphas = [10 ** p for p in [4, 3, 2]]  # [2, 3, 4, 5], [5, 4, 3, 2]\n",
    "num_sp_samples = 10  # 10\n",
    "num_cv_splits = 5  # 5\n",
    "power = 1\n",
    "\n",
    "global_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOSTON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get Boston data.\n",
    "dataset_name = 'boston'\n",
    "dataset = load_boston()\n",
    "\n",
    "data = np.concatenate((dataset.data, dataset.target.reshape(-1, 1)), axis=1)\n",
    "data = scale_01(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Optional subsetting for troubleshooting.\n",
    "#data = data[np.random.choice(len(data), 200, replace=False)]\n",
    "#data = data[:, [4, 5, 6, 13]]\n",
    "#plot_nd(data, title='Data, scaled [0, 1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of parameters to test.\n",
    "train_size = int(len(data) * (num_cv_splits - 1) / num_cv_splits)\n",
    "percent_num_supp = [0.02, 0.05, 0.1]\n",
    "num_supp_list = [int(train_size * i) for i in percent_num_supp]\n",
    "print('Train size: {}'.format(train_size))\n",
    "print('num_supp_list: {}'.format(num_supp_list))\n",
    "#num_supp = [5]\n",
    "max_iter = 301 #501  # 301\n",
    "lr = 0.5 #0.01         # 0.5\n",
    "burnin = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run experiments and print results.\n",
    "if run_boston:\n",
    "    #np.random.seed(123)\n",
    "    results = run_experiments(dataset_name, data, num_supp_list, alphas,\n",
    "                              max_iter, lr, num_sp_samples, burnin=burnin,\n",
    "                              num_cv_splits=num_cv_splits, power=power)\n",
    "    \n",
    "    global_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if run_boston:\n",
    "    plot_final_results(results, 'boston', percent_num_supp, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIABETES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get diabetes data.\n",
    "dataset_name = 'diabetes'\n",
    "dataset = load_diabetes()\n",
    "\n",
    "data = np.concatenate((dataset.data, dataset.target.reshape(-1, 1)), axis=1)\n",
    "data = scale_01(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Optional subsetting for troubleshooting.\n",
    "#data = data[np.random.choice(len(data), 100, replace=False)]\n",
    "data = data[:, [4, 6, 7, 10]]\n",
    "#plot_nd(data, title='Data, scaled [0, 1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of parameters to test.\n",
    "train_size = int(len(data) * (num_cv_splits - 1) / num_cv_splits) \n",
    "percent_num_supp = [0.02, 0.05, 0.1]\n",
    "num_supp_list = [int(train_size * i) for i in percent_num_supp]\n",
    "max_iter = 301\n",
    "lr = 0.5\n",
    "burnin = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run experiments and print results.\n",
    "if run_diabetes:\n",
    "    #np.random.seed(123)\n",
    "    results = run_experiments(dataset_name, data, num_supp_list, alphas,\n",
    "                              max_iter, lr, num_sp_samples,\n",
    "                              num_cv_splits=num_cv_splits, power=power)\n",
    "    \n",
    "    global_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_diabetes:\n",
    "    plot_final_results(results, 'diabetes', percent_num_supp, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## CALIFORNIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get diabetes data.\n",
    "dataset_name = 'california'\n",
    "dataset = fetch_california_housing()\n",
    "\n",
    "data = np.concatenate((dataset.data, dataset.target.reshape(-1, 1)), axis=1)\n",
    "data = scale_01(data)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Optional subsetting for troubleshooting.\n",
    "data = data[np.random.choice(len(data), 500, replace=False)]\n",
    "data = data[:, [1, 6, 7, 8]]\n",
    "#plot_nd(data, title='Data, scaled [0, 1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of parameters to test.\n",
    "train_size = int(len(data) * (num_cv_splits - 1) / num_cv_splits) \n",
    "percent_num_supp = [0.02, 0.05, 0.1]\n",
    "num_supp_list = [int(train_size * i) for i in percent_num_supp]\n",
    "max_iter = 501\n",
    "lr = 1.\n",
    "burnin = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run experiments and print results.\n",
    "if run_california:\n",
    "    #np.random.seed(123)\n",
    "    results = run_experiments(dataset_name, data, num_supp_list, alphas,\n",
    "                              max_iter, lr, num_sp_samples,\n",
    "                              num_cv_splits=num_cv_splits, power=power)\n",
    "\n",
    "    global_results.extend(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_california:\n",
    "    plot_final_results(results, 'california', percent_num_supp, alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and plot global results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to file.\n",
    "import json\n",
    "gr_as_dict = [row._asdict() for row in global_results]\n",
    "with open('../output/regression_global_results.json', 'w') as outfile:\n",
    "    json.dump(gr_as_dict, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
