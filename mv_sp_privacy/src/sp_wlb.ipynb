{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support points with Weighted Likelihood Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "from scipy.spatial.distance import pdist\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sp_utils import (\n",
    "    get_support_points,\n",
    "    scatter_and_hist,\n",
    "    eval_uncertainty)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "print(sys.version)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set global constants.          # main1()\n",
    "IS_TF = True                     # Boolean. Use TensorFlow rather than analytical gradients.\n",
    "DATA_SOURCE = 'balog'            # ['balog', 'gaussian', 'gamma', 'beta']\n",
    "MAX_ITER = 301                  # Num iterations in each support point optimization. [301]\n",
    "LR = 0.01                         # Energy optimization learning rate. [5e-3]\n",
    "\n",
    "ENERGY_POWER = 2.                # Power for energy distance kernel.\n",
    "M = 500                          # Number of data points.\n",
    "N = 25                           # Number of support points.\n",
    "DIM = 2                          # Dimension of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load and Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if DATA_SOURCE == 'balog':\n",
    "    os.system('python data.py {} {}'.format(M, DIM))\n",
    "    \n",
    "    # Note that Balog's N is our M.\n",
    "    DATA_PATH = '../data/mixture_of_Gaussians_N{}_D{}.npz'. format(M, DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data.\n",
    "if DATA_SOURCE == 'balog':\n",
    "    data = np.load(DATA_PATH)\n",
    "    x = data['X_private']\n",
    "    mus = data['MUS']\n",
    "    weights = data['WEIGHTS']\n",
    "    sigma_data = data['SIGMA_DATA']\n",
    "    assert (M, DIM) == np.shape(x), 'Balog data dims do not match global params.'\n",
    "    print('Loaded M={} data points with dimension DIM={}'.format(M, DIM))\n",
    "    plt.scatter(x[:, 0], x[:, 1], c=\"gray\", alpha=0.3,\n",
    "                label='data')\n",
    "    #plt.scatter(mus[:, 0], mus[:, 1], c='green', alpha=1, s=700*weights,\n",
    "    #            label='true centroids')\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title('Data')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run Support Point Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute m support points on full set.\n",
    "y_opt, e_opt = get_support_points(x, N, MAX_ITER, LR,\n",
    "                                  is_tf=IS_TF, clip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sample many sets of support points with WLB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute N support points from full set x.\n",
    "num_samples = 2\n",
    "y_opt_all = np.zeros((num_samples, N, DIM))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    y_opt, e_opt = get_support_points(x, N, MAX_ITER, LR, is_tf=IS_TF,\n",
    "                                      do_wlb=True)\n",
    "    y_opt_all[i] = y_opt\n",
    "\n",
    "y_all = np.concatenate(y_opt_all, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter_and_hist(x, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Uncertainty quantification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$N(0, 1), Exp(1), Beta(2, 4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "norm01_fn = lambda M: np.random.normal(loc=0, scale=1, size=(M, DIM))\n",
    "exp1_fn = lambda M: np.random.exponential(scale=1, size=(M, DIM))\n",
    "beta24_fn = lambda M: np.random.beta(a=2, b=4, size=(M, DIM))\n",
    "\n",
    "M = 500\n",
    "N = 25\n",
    "LR = 5e-3\n",
    "MAX_ITER = 1001\n",
    "num_draws = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print('Making samples of size {}'.format(N * num_draws))\n",
    "#eval_uncertainty(norm01_fn, M, N, DIM, 0.01, 301, IS_TF, num_draws, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print('Making samples of size {}'.format(N * num_draws))\n",
    "#eval_uncertainty(beta24_fn, M, N, DIM, 0.01, 301, IS_TF, num_draws, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Making samples of size {}'.format(N * num_draws))\n",
    "eval_uncertainty(exp1_fn, M, N, DIM, 0.01, 1001, IS_TF, num_draws, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
