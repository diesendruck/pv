{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentially Private Moment Discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configs that worked:\n",
    "- datadim=1, kmmd, k=100, width=3, depth=2, zdim=2, opt=rmsprop, elu, lr=5e-3, bs=64\n",
    "- datadim=1, cmd, k=5, width=3, depth=2, zdim=2, opt=rmsprop, elu, lr=5e-3, bs=64\n",
    "- datadim=2, cmd, k=5, width=3, depth=2, zdim=2, opt=rmsprop, elu, lr=5e-3, bs=64, \"two_gaussians\"\n",
    "- datadim=2, cmd, k=10, width=3, depth=2, zdim=2, opt=rmsprop, elu, lr=5e-3, bs=64, \"two_gaussians\"\n",
    "- data_file='yangmed.csv', kmmd, k=5, width=100, depth=3, zdim=64, opt=rmsprop, elu, lr=5e-3, bs=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configs that worked, but only for marginals:\n",
    "- data_file='yangmed.csv', cmd, k=10, width=100, depth=3, zdim=64, opt=rmsprop, elu, lr=5e-3, bs=256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter config.\n",
    "jupyter_verbose = True\n",
    "extra_verbose = False\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Model config.\n",
    "model_type = 'cmd_gan'\n",
    "cmd_variation = 'cmd'  # {'onetime_noisy', 'onetime_noisy_joint', 'dp_sgd', 'mmd', 'kmmd', cmd', 'ncmd', 'ncmd_jmd'}\n",
    "do_cmd_taylor_weights = False\n",
    "k_moments = 5\n",
    "width = 3\n",
    "depth = 2\n",
    "z_dim = 2\n",
    "optimizer = 'rmsprop'\n",
    "activation = tf.nn.elu\n",
    "learning_rate = 5e-3\n",
    "lr_update_step = 10000\n",
    "lr_minimum = 1e-6\n",
    "\n",
    "#data_file = 'yangmed.csv'\n",
    "data_file = ''\n",
    "data_dim = 2\n",
    "#clip_unnormed = np.array([[2., 8.5], [-1., 6.]])  # Must be 2d array of floats.\n",
    "clip_unnormed = None\n",
    "if clip_unnormed is not None:\n",
    "    assert clip_unnormed.shape == (data_dim, 2)\n",
    "\n",
    "sigma = 1\n",
    "laplace_eps = 1.0  # 1.0\n",
    "default_gradient_l2norm_bound = 4.0  # 4.0\n",
    "sgd_target_eps = [0.125, 0.25, 0.5, 1., 2., 4., 8.]  # [0.125, 0.25, 0.5, 1., 2., 4., 8.]\n",
    "sgd_eps = 1.0  # 1.0\n",
    "sgd_delta = 4.0  # 4.0\n",
    "sgd_sigma = 4.0  # 4.0\n",
    "\n",
    "data_num_init = 5000\n",
    "percent_train = 0.9\n",
    "batch_size = 64\n",
    "gen_num = 64\n",
    "log_step = 100\n",
    "max_step = 200000\n",
    "\n",
    "tag = 'test'\n",
    "load_existing = False\n",
    "\n",
    "args = [model_type, cmd_variation, default_gradient_l2norm_bound,\n",
    "        laplace_eps, sgd_target_eps, sgd_eps, sgd_delta, sgd_sigma,\n",
    "        data_num_init, data_dim, percent_train, batch_size, gen_num,\n",
    "        width, depth, z_dim, log_step, max_step, learning_rate,\n",
    "        lr_update_step, lr_minimum, optimizer, data_file, k_moments,\n",
    "        sigma, tag, load_existing, activation]\n",
    "args = [str(a) for a in args]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This script runs differential privacy SGD (moment accountant) on a GAN\n",
    "    with moment discrepancy loss.\n",
    "\"\"\"\n",
    "import argparse\n",
    "from time import time\n",
    "import os\n",
    "import pdb\n",
    "import shutil\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.stats import truncnorm, pearsonr\n",
    "import tensorflow as tf\n",
    "layers = tf.layers\n",
    "\n",
    "#sys.path.append('/home/maurice/mmd')\n",
    "from mmd_utils import (compute_mmd, compute_kmmd, compute_cmd,\n",
    "                       compute_joint_moment_discrepancy,\n",
    "                       compute_noncentral_moment_discrepancy,\n",
    "                       compute_moments, compute_central_moments,\n",
    "                       compute_kth_central_moment, MMD_vs_Normal_by_filter,\n",
    "                       dp_sensitivity_to_expectation)\n",
    "\n",
    "from dp_optimizer import DPGradientDescentOptimizer\n",
    "from sanitizer import AmortizedGaussianSanitizer, ClipOption\n",
    "from accountant import GaussianMomentsAccountant, EpsDelta\n",
    "from utils import NetworkParameters, LayerParameters, BuildNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mm(arr):\n",
    "    \"\"\"Prints min and max of array.\"\"\"\n",
    "    print('  {}, {}'.format(np.min(arr), np.max(arr)))\n",
    "\n",
    "\n",
    "def get_random_z(gen_num, z_dim, for_training=True):\n",
    "    \"\"\"Generates 2d array of noise input data.\"\"\"\n",
    "    #return np.random.uniform(size=[gen_num, z_dim],\n",
    "    #                         low=-1.0, high=1.0)\n",
    "    if for_training:\n",
    "        return np.random.normal(size=[gen_num, z_dim])\n",
    "    else:\n",
    "        return truncnorm.rvs(-3, 3, size=[gen_num, z_dim])\n",
    "\n",
    "\n",
    "def reduce_var(x, axis=None, keepdims=False):\n",
    "    \"\"\"Variance of a tensor, alongside the specified axis.\n",
    "\n",
    "    Args:\n",
    "        x: A tensor or variable.\n",
    "        axis: An integer, the axis to compute the variance.\n",
    "        keepdims: A boolean, whether to keep the dimensions or not.\n",
    "            If `keepdims` is `False`, the rank of the tensor is reduced\n",
    "            by 1. If `keepdims` is `True`,\n",
    "            the reduced dimension is retained with length 1.\n",
    "\n",
    "    Returns:\n",
    "        A tensor with the variance of elements of `x`.\n",
    "    \"\"\"\n",
    "    m = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "    devs_squared = tf.square(x - m)\n",
    "    return tf.reduce_mean(devs_squared, axis=axis, keepdims=keepdims)\n",
    "\n",
    "\n",
    "def reduce_std(x, axis=None, keepdims=False):\n",
    "    \"\"\"Standard deviation of a tensor, alongside the specified axis.\n",
    "\n",
    "    Args:\n",
    "        x: A tensor or variable.\n",
    "        axis: An integer, the axis to compute the standard deviation.\n",
    "        keepdims: A boolean, whether to keep the dimensions or not.\n",
    "            If `keepdims` is `False`, the rank of the tensor is reduced\n",
    "            by 1. If `keepdims` is `True`,\n",
    "            the reduced dimension is retained with length 1.\n",
    "\n",
    "    Returns:\n",
    "        A tensor with the standard deviation of elements of `x`.\n",
    "    \"\"\"\n",
    "    return tf.sqrt(reduce_var(x, axis=axis, keepdims=keepdims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and autoencoder TF functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(x, width, activation, batch_residual=False, use_bias=False, name=None,\n",
    "          bounds=None):\n",
    "    \"\"\"Wrapper on fully connected TensorFlow layer.\n",
    "    \n",
    "    Args:\n",
    "      x: Layer input.\n",
    "      width: Width of output layer.\n",
    "      activation: TensorFlow activation function.\n",
    "      batch_residual: Flag to use batch residual output.\n",
    "      use_bias: Flag to use bias in fully connected layer.\n",
    "      bounds: List of min and max scalar values to clip to. [min, max]\n",
    "    \"\"\"\n",
    "    if batch_residual:\n",
    "        option = 1\n",
    "        if option == 1:\n",
    "            x_ = layers.dense(x, width, activation=activation, use_bias=use_bias)\n",
    "            if bounds is not None:\n",
    "                x_ = tf.clip_by_value(x_, bounds[0], bounds[1])\n",
    "            r = layers.batch_normalization(x_) + x\n",
    "        elif option == 2:\n",
    "            x_newdim = layers.dense(x, width, activation=activation,\n",
    "                                    use_bias=use_bias, name=name)\n",
    "            x_newdim = layers.batch_normalization(x_newdim)\n",
    "            x_newdim = tf.nn.relu(x_newdim)\n",
    "            x = layers.dense(x_newdim, width, activation=activation,\n",
    "                              use_bias=use_bias, name=name)\n",
    "            x = layers.batch_normalization(x)\n",
    "            x = tf.nn.relu(x)\n",
    "            r = x + x_newdim\n",
    "    else:\n",
    "        x_ = layers.dense(x, width, activation=activation, use_bias=use_bias)\n",
    "        if bounds is not None:\n",
    "            x_ = tf.clip_by_value(x_, bounds[0], bounds[1])\n",
    "        r = layers.batch_normalization(x_)\n",
    "    return r\n",
    "\n",
    "\n",
    "def fully_connected(x, num_units, scope):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:\n",
    "        w = tf.get_variable(\n",
    "            \"weights\",\n",
    "            [x.shape[1], num_units],\n",
    "            initializer=tf.truncated_normal_initializer(\n",
    "                stddev=1.0 / np.sqrt(num_units)))\n",
    "        b = tf.get_variable(\n",
    "            \"biases\",\n",
    "            [num_units],\n",
    "            initializer=tf.constant_initializer(0.0))\n",
    "        return tf.matmul(x, w) + b\n",
    "\n",
    "\n",
    "def generator(z_in, width=3, depth=3, activation=tf.nn.elu, out_dim=2,\n",
    "              reuse=False, bounds=None):\n",
    "    \"\"\"Decodes. Generates output, given noise input.\"\"\"\n",
    "    bounds = None \n",
    "    if bounds == None:\n",
    "        print('No bounds on Generator')\n",
    "    with tf.variable_scope('generator', reuse=reuse) as vs_g:\n",
    "        x = dense(z_in, width, activation=activation, bounds=bounds,\n",
    "                  batch_residual=False, use_bias=False)\n",
    "\n",
    "        for idx in range(depth - 1):\n",
    "            # TODO: Should this use batch resid, and is it defined properly?\n",
    "            x = dense(x, width, activation=activation, bounds=bounds,\n",
    "                      batch_residual=False, use_bias=False)\n",
    "\n",
    "        out = dense(x, out_dim, activation=None, bounds=bounds,\n",
    "                    batch_residual=False, use_bias=False)\n",
    "    vars_g = tf.contrib.framework.get_variables(vs_g)\n",
    "    return out, vars_g\n",
    "\n",
    "\n",
    "def generator_v2(inputs, network_parameters):\n",
    "    #with tf.variable_scope('generator', reuse=reuse) as vs_g:\n",
    "    #with tf.variable_scope('generator_v2', reuse=tf.AUTO_REUSE):\n",
    "        #out, _, training_params = BuildNetwork(inputs, network_parameters)\n",
    "\n",
    "    num_inputs = network_parameters.input_size\n",
    "    outputs = inputs\n",
    "\n",
    "    # First hidden layer.\n",
    "    h0_params = network_parameters.layer_parameters[0]\n",
    "    outputs_fc = fully_connected(outputs, h0_params.num_units, h0_params.name)\n",
    "    outputs_bn = (outputs_fc - tf.reduce_mean(outputs_fc, axis=0)) / (\n",
    "        reduce_std(outputs_fc, axis=0))\n",
    "    outputs = tf.nn.elu(outputs_bn)\n",
    "\n",
    "    # Remainder of hidden layers with batch residual.\n",
    "    for h_params in network_parameters.layer_parameters[1:]:\n",
    "        if 'hidden' in h_params.name:\n",
    "            num_units = h_params.num_units\n",
    "            outputs_fc = fully_connected(outputs, num_units, h_params.name)\n",
    "            outputs_bn = (outputs_fc - tf.reduce_mean(outputs_fc, axis=0)) / (\n",
    "                reduce_std(outputs_fc, axis=0))\n",
    "            outputs = tf.nn.elu(outputs_bn)\n",
    "            #outputs = outputs_bn + outputs\n",
    "\n",
    "    # Do final layer.\n",
    "    hl_params = network_parameters.layer_parameters[-1]\n",
    "    num_units = hl_params.num_units\n",
    "    outputs_fc = fully_connected(outputs, num_units, hl_params.name)\n",
    "    outputs = outputs_fc\n",
    "\n",
    "    return outputs \n",
    "\n",
    "\n",
    "def autoencoder(x, width=3, depth=3, activation=tf.nn.elu, z_dim=3,\n",
    "                reuse=False, normed_weights=False, normed_encs=False,\n",
    "                bounds=None):\n",
    "    \"\"\"Autoencodes input via a bottleneck layer h.\"\"\"\n",
    "    out_dim = x.shape[1]\n",
    "    with tf.variable_scope('encoder', reuse=reuse) as vs_enc:\n",
    "        x = dense(x, width, activation=activation, bounds=bounds)\n",
    "\n",
    "        for idx in range(depth - 1):\n",
    "            # TODO: Should this use batch resid, and is it defined properly?\n",
    "            x = dense(x, width, activation=activation, batch_residual=True,\n",
    "                      bounds=bounds)\n",
    "\n",
    "        h = dense(x, z_dim, activation=None, bounds=bounds)\n",
    "\n",
    "    with tf.variable_scope('decoder', reuse=reuse) as vs_dec:\n",
    "\n",
    "        x = dense(h, width, activation=activation, name='hidden',\n",
    "                  bounds=bounds)\n",
    "\n",
    "        for idx in range(depth - 1):\n",
    "            x = dense(x, width, activation=activation, batch_residual=True,\n",
    "                      bounds=bounds)\n",
    "\n",
    "        ae = dense(x, out_dim, activation=None, bounds=bounds)\n",
    "\n",
    "    vars_enc = tf.contrib.framework.get_variables(vs_enc)\n",
    "    vars_dec = tf.contrib.framework.get_variables(vs_dec)\n",
    "\n",
    "    return h, ae, vars_enc, vars_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and normalize raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_num(v):\n",
    "    \"\"\"Returns Tukey's five number summary (minimum, lower-hinge, median, upper-hinge, maximum) for the input vector, a list or array of numbers based on 1.5 times the interquartile distance\"\"\"\n",
    "    try:\n",
    "        np.sum(v)\n",
    "    except TypeError:\n",
    "        print('Error: you must provide a list or array of only numbers')\n",
    "    q1 = np.percentile(v, 25)\n",
    "    q3 = np.percentile(v, 75)\n",
    "    iqd = q3 - q1\n",
    "    md = np.median(v)\n",
    "    whisker = 1.5 * iqd\n",
    "    return np.min(v), q1, md, q3, np.max(v)\n",
    "\n",
    "\n",
    "def load_normed_data(data_num, percent_train, log_dir, clip_unnormed=None,\n",
    "                     clip=None, data_file=None):\n",
    "    \"\"\"Generates data, and returns it normalized, along with helper objects.\"\"\"\n",
    "    \n",
    "    # Load data.\n",
    "    if data_file not in ['', None]:\n",
    "        if data_file.endswith('npy'):\n",
    "            data_raw = np.load(data_file)\n",
    "        elif data_file.endswith('txt'):\n",
    "            data_raw = np.loadtxt(open(data_file, 'rb'), delimiter=' ')\n",
    "        elif data_file.endswith('csv'):\n",
    "            data_raw = np.loadtxt(open(data_file, 'rb'), delimiter=',')\n",
    "        \n",
    "        #########################################################################\n",
    "        # YANG MED DATA (begin)\n",
    "        \n",
    "        if 'yang' in data_file:\n",
    "            d = np.loadtxt(open(data_file, 'rb'), delimiter=',')\n",
    "            print('Using first 10k of data.')\n",
    "            d = d[:20000]\n",
    "            d = np.delete(d, 226, axis=0)  # Error.\n",
    "            d = np.random.permutation(d)\n",
    "            num_rows = d.shape[0]\n",
    "            num_cols = d.shape[1]\n",
    "\n",
    "            # Separate train and test data.\n",
    "            num_train = int(percent_train * num_rows)\n",
    "            d_train = d[:num_train]\n",
    "            d_test = d[num_train:]\n",
    "\n",
    "            print('\\nRaw data:')\n",
    "            for i in range(num_cols):\n",
    "                print('{: >8},{: >8},{: >8},{: >8},{: >8}'.format(\n",
    "                    *five_num(d[:,i])))\n",
    "\n",
    "            binary_cols = []\n",
    "            for col in range(num_cols):\n",
    "                col_data = d[:, col]\n",
    "                if np.array_equal(col_data, col_data.astype(bool)):\n",
    "                    binary_cols.append(col)\n",
    "            print('binary_cols={}'.format(binary_cols))\n",
    "\n",
    "            # Don't standardize binary vars.\n",
    "            #mean_mask = np.array([1] * num_cols)\n",
    "            mean_mask = np.ones(num_cols)\n",
    "            mean_mask[binary_cols] = 0.  # Do not subtract mean from binary vars.\n",
    "            mean_vec = d_train.mean(0) * mean_mask\n",
    "            std_vec = d_train.std(0)\n",
    "            std_vec[binary_cols] = 1.  # Do not divide by std for binary vars.\n",
    "            data = (d_train - mean_vec) / std_vec\n",
    "            data_test = (d_test - mean_vec) / std_vec\n",
    "\n",
    "            #print('\\nStandardized data:')\n",
    "            #for i in range(data.shape[1]):\n",
    "            #    print('{: >8},{: >8},{: >8},{: >8},{: >8}'.format(\n",
    "            #        *five_num(data[:,i])))\n",
    "                \n",
    "            # Save copy of raw data.\n",
    "            np.save(os.path.join(log_dir, 'data_raw.npy'), d)\n",
    "\n",
    "            # Set up a few helpful constants.\n",
    "            data_num = d_train.shape[0]\n",
    "            data_test_num = d_test.shape[0]\n",
    "            out_dim = data.shape[1]\n",
    "\n",
    "            # Adjust clip values to use the tighest interval.\n",
    "            # For clip lows, use greater of clip_low and min(data).\n",
    "            # For clip highs, use lesser of clip_high and max(data).\n",
    "            if clip_unnormed is not None:\n",
    "                clip_unnormed[:, 0] = np.max(np.vstack(\n",
    "                    (clip_unnormed[:, 0], np.min(d, axis=0))), axis=0)\n",
    "                clip_unnormed[:, 1] = np.min(np.vstack(\n",
    "                    (clip_unnormed[:, 1], np.max(d, axis=0))), axis=0)\n",
    "                clip = ((clip_unnormed - mean_vec.reshape(-1, 1)) / std_vec.reshape(-1, 1))\n",
    "\n",
    "            return (data, data_test, data_num, data_test_num, out_dim,\n",
    "                    mean_vec, std_vec, clip)\n",
    "\n",
    "        # YANG MED DATA (end)\n",
    "        #################################################################################\n",
    "\n",
    "        \n",
    "    else:\n",
    "        if data_dim == 1:\n",
    "            \n",
    "            data_raw = np.zeros((data_num, 1))\n",
    "            for i in range(data_num):\n",
    "                # Pick a Gaussian, then generate from that Gaussian.\n",
    "                i_cluster = np.random.binomial(1, 0.1)  # NOTE: Setting p=0/p=1 chooses one cluster.\n",
    "                if i_cluster == 0:\n",
    "                    data_raw[i] = np.random.normal(5, 0.5)\n",
    "                else:\n",
    "                    data_raw[i] = np.random.normal(6.5, 0.5)\n",
    "            \n",
    "            # Plot clipped version of data.\n",
    "            if clip_unnormed is not None:\n",
    "                data_clipped = np.clip(data_raw, \n",
    "                                       clip_unnormed[0][0], clip_unnormed[0][1])            \n",
    "                fig, ax = plt.subplots()\n",
    "                ax.hist(data_raw, density=True, bins=30, color='gray', alpha=0.3,\n",
    "                        label='data')\n",
    "                ax.hist(data_clipped, density=True, bins=30, color='blue', alpha=0.3,\n",
    "                        label='clipped')\n",
    "                plt.legend()\n",
    "                filepath = os.path.join(plot_dir, '{}.png'.format('data_clipped'))\n",
    "                plt.savefig(filepath)\n",
    "                plt.close(fig)\n",
    "                if jupyter_verbose:\n",
    "                    display(Image(filename=filepath))\n",
    "                               \n",
    "        elif data_dim == 2:\n",
    "            \n",
    "            design = 'two_gaussians'\n",
    "            print('Using data set {}'.format(design))\n",
    "            if design == 'two_gaussians':\n",
    "                data_raw = np.zeros((data_num, 2))\n",
    "                for i in range(data_num):\n",
    "                    if np.random.binomial(1, 0.5):  # 2nd param controls mixture.\n",
    "                        data_raw[i] = \\\n",
    "                            np.random.multivariate_normal([4., 2.], [[0.5, 0.], [0., 0.5]], 1)\n",
    "                    else:\n",
    "                        data_raw[i] = \\\n",
    "                            np.random.multivariate_normal([6., 4.], [[0.5, 0.1], [0.1, 0.5]], 1)\n",
    "\n",
    "            elif design == 'noisy_sin':\n",
    "                x = np.linspace(0, 10, data_num)\n",
    "                y = np.sin(x) + np.random.normal(0, 0.5, len(x))\n",
    "                data_raw = np.hstack((np.expand_dims(x, axis=1),\n",
    "                                      np.expand_dims(y, axis=1)))\n",
    "                data_raw = data_raw[np.random.permutation(data_num)]\n",
    "            elif design == 'uniform':\n",
    "                data_raw = np.zeros((data_num, 2))\n",
    "                for i in range(data_num):\n",
    "                    if np.random.binomial(1, 0.5):\n",
    "                        x_sample = np.random.uniform(0, 1)\n",
    "                        y_sample = np.random.uniform(0, 1)\n",
    "                    else:\n",
    "                        x_sample = np.random.uniform(-1, 0)\n",
    "                        y_sample = np.random.uniform(-1, 0)\n",
    "                    data_raw[i] = [x_sample, y_sample]\n",
    "            \n",
    "            # Plot clipped version of data.\n",
    "            if clip_unnormed is not None:\n",
    "                data_clipped = np.zeros(data_raw.shape)\n",
    "                data_clipped[:, 0] = np.clip(data_raw[:, 0],\n",
    "                                             clip_unnormed[0][0],\n",
    "                                             clip_unnormed[0][1])\n",
    "                data_clipped[:, 1] = np.clip(data_raw[:, 1],\n",
    "                                             clip_unnormed[1][0],\n",
    "                                             clip_unnormed[1][1])\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.scatter(*zip(*data_raw), color='gray',\n",
    "                                 alpha=0.2, label='raw')\n",
    "                ax.scatter(*zip(*data_clipped), color='green',\n",
    "                                 alpha=0.2, label='clipped')\n",
    "                plt.legend()\n",
    "                filepath = os.path.join(plot_dir, '{}.png'.format('data_clipped'))\n",
    "                plt.savefig(filepath)\n",
    "                plt.close(fig)\n",
    "                if jupyter_verbose:\n",
    "                    display(Image(filename=filepath))\n",
    "                    \n",
    "    # Save copy of raw data.\n",
    "    np.save(os.path.join(log_dir, 'data_raw.npy'), data_raw)    \n",
    "\n",
    "    \n",
    "    # First split data into Train/Test, then normalize both.\n",
    "    # Split.\n",
    "    num_train = int(percent_train * data_raw.shape[0])\n",
    "    data_raw_train = data_raw[:num_train]\n",
    "    data_raw_test = data_raw[num_train:]\n",
    "    # Normalize (based only on training data).\n",
    "    data_raw_train_mean = np.mean(data_raw_train, axis=0)\n",
    "    data_raw_train_std = np.std(data_raw_train, axis=0)\n",
    "    data = (data_raw_train - data_raw_train_mean) / data_raw_train_std  # Normed training data\n",
    "    data_test = (data_raw_test - data_raw_train_mean) / data_raw_train_std  # Normed test data\n",
    "    \n",
    "    \n",
    "    # Set up a few helpful constants.\n",
    "    data_num = data.shape[0]\n",
    "    data_test_num = data_test.shape[0]\n",
    "    out_dim = data.shape[1]\n",
    "    \n",
    "    # Adjust clip values to use the tighest interval.\n",
    "    # For clip lows, use greater of clip_low and min(data).\n",
    "    # For clip highs, use lesser of clip_high and max(data).\n",
    "    if clip_unnormed is not None:\n",
    "        clip_unnormed[:, 0] = np.max(np.vstack(\n",
    "            (clip_unnormed[:, 0], np.min(data_raw, axis=0))), axis=0)\n",
    "        clip_unnormed[:, 1] = np.min(np.vstack(\n",
    "            (clip_unnormed[:, 1], np.max(data_raw, axis=0))), axis=0)\n",
    "        clip = ((clip_unnormed - data_raw_train_mean.reshape(-1, 1)) / \n",
    "                data_raw_train_std.reshape(-1, 1))\n",
    "\n",
    "    return (data, data_test, data_num, data_test_num, out_dim,\n",
    "            data_raw_train_mean, data_raw_train_std, clip)\n",
    "\n",
    "\n",
    "def unnormalize(data_normed, data_raw_mean, data_raw_std):\n",
    "    \"\"\"Unnormalizes data based on mean and std.\"\"\"\n",
    "    return data_normed * data_raw_std + data_raw_mean\n",
    "\n",
    "\n",
    "def print_baseline_moment_stats(d_normed, d_mean, d_std, k_moments):\n",
    "    \"\"\"Compute baseline statistics on moments for data set.\"\"\"\n",
    "    d = d_normed * d_std + d_mean\n",
    "    baseline_moments = compute_moments(d, k_moments)\n",
    "    for j in range(k_moments):\n",
    "        print('Moment {}: {}'.format(j+1, baseline_moments[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity and noisy moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fixed_batches(data, batch_size):\n",
    "    # Partition data into fixed batches of batch_size.\n",
    "    data_num = data.shape[0]\n",
    "    data_dim = data.shape[1]\n",
    "    fixed_batches = np.array(\n",
    "        [data[i:i + batch_size] for i in range(0, len(data), batch_size)])\n",
    "    fixed_batches = np.array(\n",
    "        [b for b in fixed_batches if len(b) == batch_size])\n",
    "    return fixed_batches\n",
    "\n",
    "def compute_sensitivities(data, batch_size, k_moments, clip=None):\n",
    "    \"\"\"Computes all forms of sensitivity.\n",
    "    \n",
    "    Sensitivities are based on batch_size, number of moments, and \n",
    "    clipping.\n",
    "    \n",
    "        clip: [[dim1_low, dim1_high],\n",
    "               [dim2_low, dim2_high],\n",
    "                ...,\n",
    "               [dimD_low, dimD_high]]\n",
    "        moment_sensitivities:\n",
    "              [[m1_dim1, ..., m1_dimD],\n",
    "               [m2_dim1, ..., m2_dimD],\n",
    "                ...,\n",
    "               [mk_dim1, ..., mk_dimD],]\n",
    "    \"\"\"\n",
    "    # Verify dimensions of clip. Should have a min and max for each data dim.\n",
    "    if clip is not None:\n",
    "        assert clip.shape == (data.shape[1], 2), 'clip must be shape (data_dim, 2)'\n",
    "    \n",
    "    # MOMENT sensitivities for entire data set.\n",
    "    moment_sensitivities = np.zeros((k_moments, data.shape[1]))\n",
    "    if clip is not None:\n",
    "        for k in range(1, k_moments + 1):\n",
    "            mk_sens = np.power(np.max(np.abs(clip), axis=1), k) / batch_size  # D-dimensional \n",
    "            moment_sensitivities[k - 1, :] = mk_sens\n",
    "    else:\n",
    "        for k in range(1, k_moments + 1):\n",
    "            mk_sens = np.power(np.max(np.abs(data), axis=0), k) / batch_size  # D-dimensional\n",
    "            moment_sensitivities[k - 1, :] = mk_sens \n",
    "\n",
    "    # JOINT-MOMENT sensitivities for entire data set.\n",
    "    jmoment_sensitivities = np.zeros((k_moments, 1))\n",
    "    if clip is not None:\n",
    "        for k in range(1, k_moments + 1):\n",
    "            jmk_sens = np.power(np.prod(np.max(np.abs(clip), axis=1)), k) / batch_size  # 1-dimensional.\n",
    "            jmoment_sensitivities[k - 1, :] = jmk_sens\n",
    "    else:\n",
    "        for k in range(1, k_moments + 1):\n",
    "            # For medical data, only compute joint sensitivity over non-binary cols.\n",
    "            if 'yang' in data_file:\n",
    "                jmk_sens = np.power(np.max(\n",
    "                    np.abs(np.prod(data[:, :17], axis=1)), axis=0), k) / batch_size\n",
    "                jmoment_sensitivities[k - 1, :] = jmk_sens\n",
    "            else:\n",
    "                jmk_sens = np.power(np.max(\n",
    "                    np.abs(np.prod(data, axis=1)), axis=0), k) / batch_size\n",
    "                jmoment_sensitivities[k - 1, :] = jmk_sens\n",
    "\n",
    "            \n",
    "    # Compute QUANTILE sensitivities for the entire data set, i.e. the\n",
    "    # the difference between the quantile value and the next value in a sorted\n",
    "    # array.\n",
    "    n = data.shape[0]\n",
    "    quantiles = [0.01, 0.1, 0.5, 0.9, 0.99]\n",
    "    global_quantile_values = np.zeros((data_dim, len(quantiles)))\n",
    "    quantile_sensitivities = np.zeros((data_dim, len(quantiles)))\n",
    "    for dim in range(data_dim):\n",
    "        component_data = sorted(data[:, dim])\n",
    "        for q_i, q_v in enumerate(quantiles):\n",
    "            # Index associated with quantile.\n",
    "            index_q = int(np.ceil(n * q_v))\n",
    "            global_quantile_values[dim, q_i] = component_data[index_q] \n",
    "            quantile_sensitivities[dim, q_i] = (\n",
    "                component_data[index_q + 1] - component_data[index_q])\n",
    "\n",
    "    return (moment_sensitivities, jmoment_sensitivities,\n",
    "            global_quantile_values, quantile_sensitivities)\n",
    "\n",
    "\n",
    "def make_fbo_noisy_moments(fixed_batches, moment_sensitivities,\n",
    "                           k_moments, laplace_eps, allocation=None):\n",
    "    \"\"\"Sets up true and noisy moments per batch.\n",
    "\n",
    "    Args:\n",
    "      fixed_batches: Array of data batches, [batch_num, batch_size, data_dim].\n",
    "      moment_sensitivities: Array of sensitivities of moments,\n",
    "        [k_moments, data_dim].\n",
    "      k_moments: Integer number of moments to compute.\n",
    "      laplace_eps: Float, overall privacy budget.\n",
    "      allocation: List of values, to be normalized, that allocate the privacy\n",
    "        budget among moments.\n",
    "\n",
    "    Returns:\n",
    "      fixed_batches_onetime_noisy_moments: Array of fixed batch noisy moments,\n",
    "        according to sensitivities of each moment, [batch_num, k_moments].\n",
    "    \"\"\"\n",
    "    data_dim = fixed_batches.shape[-1]\n",
    "    fixed_batches_moments = np.zeros(\n",
    "        (len(fixed_batches), k_moments, data_dim), dtype=np.float32)\n",
    "    fixed_batches_onetime_noisy_moments = np.zeros(\n",
    "        (len(fixed_batches), k_moments, data_dim), dtype=np.float32)\n",
    "\n",
    "    # Choose allocation of budget.\n",
    "    if allocation is None:\n",
    "        allocation = [1] * k_moments\n",
    "    print('Privacy budget and allocation: {}, {}\\n'.format(\n",
    "        laplace_eps, allocation))\n",
    "    eps_ = laplace_eps * (np.array(allocation) / float(np.sum(allocation)))\n",
    "    assert len(eps_) == k_moments, 'allocation length must match moment num'\n",
    "\n",
    "    # Get moments and noisy version for each batch.\n",
    "    for batch_num, batch in enumerate(fixed_batches):\n",
    "        # Each moment within that batch.\n",
    "        raw_moments = np.zeros((k_moments, data_dim))\n",
    "        noisy_moments = np.zeros((k_moments, data_dim))\n",
    "        for k in range(1, k_moments + 1):\n",
    "            mk_sens = moment_sensitivities[k - 1]\n",
    "            # Sample laplace noise for each dimension of data -- scale\n",
    "            # param takes vector of laplace scales and outputs\n",
    "            # corresponding values.\n",
    "            mk_laplace = np.random.laplace(loc=0, scale=mk_sens/eps_[k-1])\n",
    "            mk = np.mean(np.power(batch, k), axis=0)\n",
    "            mk_noisy = mk + mk_laplace\n",
    "            raw_moments[k - 1] = mk\n",
    "            noisy_moments[k - 1] = mk_noisy\n",
    "        fixed_batches_moments[batch_num] = raw_moments\n",
    "        fixed_batches_onetime_noisy_moments[batch_num] = noisy_moments\n",
    "    print(' Sample: RAW moments')\n",
    "    print(fixed_batches_moments[0])\n",
    "    print(' Sample: NOISY moments')\n",
    "    print(fixed_batches_onetime_noisy_moments[0])\n",
    "    return fixed_batches_onetime_noisy_moments\n",
    "\n",
    "\n",
    "def make_fbo_noisy_jmoments(fixed_batches, jmoment_sensitivities,\n",
    "                            k_moments, laplace_eps, allocation=None):\n",
    "    \"\"\"Sets up true and noisy joint moments per batch.\n",
    "\n",
    "    Args:\n",
    "      fixed_batches: Array of data batches, [batch_num, batch_size, data_dim].\n",
    "      jmoment_sensitivities: Array of sensitivities of joint moments,\n",
    "        [k_moments].\n",
    "      k_moments: Integer number of moments to compute.\n",
    "      laplace_eps: Float, overall privacy budget.\n",
    "      allocation: List of values, to be normalized, that allocate the privacy\n",
    "        budget among moments.\n",
    "\n",
    "    Returns:\n",
    "      fixed_batches_onetime_noisy_jmoments: Array of fixed batch noisy joint\n",
    "        moments, according to sensitivities of each joint moment,\n",
    "        [batch_num, k_moments].\n",
    "    \"\"\"\n",
    "    data_dim = fixed_batches.shape[-1]\n",
    "    fixed_batches_jmoments = np.zeros(\n",
    "        (len(fixed_batches), k_moments), dtype=np.float32)\n",
    "    fixed_batches_onetime_noisy_jmoments = np.zeros(\n",
    "        (len(fixed_batches), k_moments), dtype=np.float32)\n",
    "\n",
    "    # Choose allocation of budget.\n",
    "    if allocation is None:\n",
    "        allocation = [1] * k_moments\n",
    "    print('Privacy budget and allocation: {}, {}\\n'.format(\n",
    "        laplace_eps, allocation))\n",
    "    eps_ = laplace_eps * (np.array(allocation) / float(np.sum(allocation)))\n",
    "    assert len(eps_) == k_moments, 'allocation length must match moment num'\n",
    "\n",
    "    # Get moments and noisy version for each batch.\n",
    "    for batch_num, batch in enumerate(fixed_batches):\n",
    "        # Each moment within that batch.\n",
    "        raw_jmoments = np.zeros(k_moments)\n",
    "        noisy_jmoments = np.zeros(k_moments)\n",
    "        for k in range(1, k_moments + 1):\n",
    "            mk_sens = jmoment_sensitivities[k - 1]  \n",
    "            # Sample laplace noise for each dimension of data -- scale\n",
    "            # param takes vector of laplace scales and outputs\n",
    "            # corresponding values.\n",
    "            mk_laplace = np.random.laplace(loc=0, scale=mk_sens/eps_[k-1])\n",
    "            mk = np.mean(np.power(np.prod(batch, axis=1), k), axis=0)\n",
    "            mk_noisy = mk + mk_laplace\n",
    "            raw_jmoments[k - 1] = mk\n",
    "            noisy_jmoments[k - 1] = mk_noisy\n",
    "        fixed_batches_jmoments[batch_num] = raw_jmoments\n",
    "        fixed_batches_onetime_noisy_jmoments[batch_num] = noisy_jmoments\n",
    "    print(' Sample: RAW jmoments')\n",
    "    print(fixed_batches_jmoments[0])\n",
    "    print(' Sample: NOISY jmoments')\n",
    "    print(fixed_batches_onetime_noisy_jmoments[0])\n",
    "    return fixed_batches_onetime_noisy_jmoments\n",
    "\n",
    "\n",
    "def make_noisy_quantiles(global_quantile_values, \n",
    "                         quantile_sensitivities,\n",
    "                         laplace_eps):\n",
    "    \"\"\"Adds Laplace noise to true, global quantile values.\n",
    "\n",
    "    Args:\n",
    "      global_quantile_values: Array of true quantile values,\n",
    "        [data_dim, num_quantiles].\n",
    "      quantile_sensitivities: Array of sensitivities of quantiles,\n",
    "        [data_dim, num_quantiles].\n",
    "      laplace_eps: Float, overall privacy budget.\n",
    "\n",
    "    Returns:\n",
    "      onetime_noisy_quantiles: Array of noisy quantiles, according to\n",
    "        sensitivities of each quantile, [batch_num, data_dim, num_quantiles].\n",
    "    \"\"\"\n",
    "    # Simple case: Just perturb global quantiles with Laplace noise.\n",
    "    num_quantiles = quantile_sensitivities.shape[-1]\n",
    "    onetime_noisy_quantiles = np.zeros(global_quantile_values.shape,\n",
    "                                       dtype=np.float32)\n",
    "\n",
    "    # Sample laplace noise for each dimension of data -- scale\n",
    "    # param takes vector of laplace scales and outputs\n",
    "    # corresponding values.\n",
    "    scale_params = quantile_sensitivities / laplace_eps\n",
    "    laplace_noise = np.random.laplace(loc=0, scale=scale_params)\n",
    "    onetime_noisy_quantiles = global_quantile_values + laplace_noise\n",
    "\n",
    "    print(' RAW quantiles')\n",
    "    print(global_quantile_values)\n",
    "    print(' NOISY quantiles')\n",
    "    print(onetime_noisy_quantiles)\n",
    "    return onetime_noisy_quantiles\n",
    "\n",
    "\n",
    "def get_noisy_mean_cov(fixed_batches, laplace_eps):\n",
    "    \"\"\"Computes noisy mean and covariance for fixed batches, based on\n",
    "    sensitivities of each across all batches.\n",
    "    \n",
    "    Args:\n",
    "      fixed_batches: NumPy array of fixed batches of inputs, of dimension\n",
    "        [num_batches, batch_size, input_dim].\n",
    "      laplace_eps: Float, differential privacy epsilon.\n",
    "\n",
    "    Returns:\n",
    "      noisy_means: NumPy array of noisy means.\n",
    "      noisy_covs: NumPy array of flattened noisy covs.\n",
    "    \"\"\"\n",
    "    num_batches = fixed_batches.shape[0]\n",
    "    batch_size = fixed_batches.shape[1]\n",
    "    input_dim = fixed_batches.shape[2]\n",
    "\n",
    "    means = np.zeros((num_batches, input_dim), dtype=np.float32)\n",
    "    covs = np.zeros((num_batches, input_dim, input_dim), dtype=np.float32)\n",
    "    noisy_means = np.zeros((num_batches, input_dim), dtype=np.float32)\n",
    "    noisy_covs = np.zeros((num_batches, input_dim, input_dim), dtype=np.float32)\n",
    "\n",
    "    # Store each batch's mean and vectorized covariance.\n",
    "    for i, b in enumerate(fixed_batches):\n",
    "        means[i] = np.mean(b, axis=0)\n",
    "        covs[i] = np.cov(b, rowvar=False)\n",
    "\n",
    "    # Compute sensitivities of means and covs.\n",
    "    # For the whole set, compute sensitivity of each moment.\n",
    "    mean_sensitivity = np.max(np.abs(means), axis=0) / batch_size\n",
    "    cov_sensitivity = (\n",
    "        np.max(\n",
    "            np.abs(\n",
    "                np.reshape(covs, [num_batches, -1])),  # Vectorized covariances.\n",
    "            axis=0) /\n",
    "        batch_size)\n",
    "\n",
    "    # Compute noisy moments for each batch.\n",
    "    for i, b in enumerate(fixed_batches):\n",
    "        natural_mean = means[i]\n",
    "        laplace_noise_mean = \\\n",
    "            np.random.laplace(loc=0, scale=mean_sensitivity/laplace_eps)\n",
    "        noisy_means[i] = natural_mean + laplace_noise_mean\n",
    "        print(natural_mean)\n",
    "        print(natural_mean + laplace_noise_mean)\n",
    "\n",
    "        natural_cov = covs[i]\n",
    "        valid_cov = False\n",
    "        tries = 0\n",
    "        tries_limit = 5\n",
    "        while not valid_cov and tries < tries_limit:\n",
    "            # Sample and laplace noise to cov. Make cov symmetric. Verify PSD.\n",
    "            laplace_noise_cov = \\\n",
    "                np.random.laplace(loc=0, scale=cov_sensitivity/laplace_eps)\n",
    "            noisy_cov_vec = np.reshape(natural_cov, [1, -1]) + laplace_noise_cov\n",
    "            noisy_cov = np.reshape(noisy_cov_vec, [input_dim, input_dim])\n",
    "            indices_lower = np.tril_indices(input_dim, -1)\n",
    "            noisy_cov[indices_lower] = noisy_cov.T[indices_lower]\n",
    "            print(natural_cov)\n",
    "            print(noisy_cov)\n",
    "            if np.all(np.linalg.eigvals(noisy_cov) > 0):\n",
    "                valid_cov = True\n",
    "            else:\n",
    "                tries += 1\n",
    "                print('----NOT PSD {}----'.format(tries))\n",
    "        noisy_covs[i] = noisy_cov\n",
    "\n",
    "    # Check that outputs are somewhat close for large eps.\n",
    "    print(' Sample: natural and noisy means')\n",
    "    print(means[:3], noisy_means[:3])\n",
    "    print(' Sample: natural and noisy covs')\n",
    "    print(covs[:3], noisy_covs[:3])\n",
    "\n",
    "    return noisy_means, noisy_covs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories for logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dirs(load_existing):\n",
    "    \"\"\"Creates directories for logs, checkpoints, and plots.\"\"\"\n",
    "    log_dir = 'logs/logs_{}'.format(tag)\n",
    "    checkpoint_dir = os.path.join(log_dir, 'checkpoints')\n",
    "    plot_dir = os.path.join(log_dir, 'plots')\n",
    "    g_out_dir = os.path.join(log_dir, 'g_out')\n",
    "    if os.path.exists(log_dir) and not load_existing:\n",
    "        shutil.rmtree(log_dir)\n",
    "    for path in [log_dir, checkpoint_dir, plot_dir, g_out_dir]:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    return log_dir, checkpoint_dir, plot_dir, g_out_dir\n",
    "\n",
    "\n",
    "def prepare_logging(log_dir, checkpoint_dir, sess):\n",
    "    \"\"\"Sets up TensorFlow logging.\"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "    summary_writer = tf.summary.FileWriter(os.path.join(log_dir, 'summary'),\n",
    "                                           sess.graph)\n",
    "    step = tf.Variable(0, name='step', trainable=False)\n",
    "    sv = tf.train.Supervisor(logdir=checkpoint_dir,\n",
    "                             is_chief=True,\n",
    "                             saver=saver,\n",
    "                             summary_op=None,\n",
    "                             summary_writer=summary_writer,\n",
    "                             save_model_secs=300,\n",
    "                             global_step=step,\n",
    "                             ready_for_local_init_op=None)\n",
    "    return saver, summary_writer\n",
    "\n",
    "\n",
    "def load_checkpoint(saver, sess, checkpoint_dir):\n",
    "    \"\"\"Restores weights from pre-trained model.\"\"\"\n",
    "    import re\n",
    "    print(' [*] Reading checkpoints...')\n",
    "    print('     {}'.format(checkpoint_dir))\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "        #counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "        #counter = int(''.join([i for i in ckpt_name if i.isdigit()]))\n",
    "        counter = int(ckpt_name.split('-')[-1])\n",
    "        print(' [*] Success to read {}'.format(ckpt_name))\n",
    "        return True, counter\n",
    "    else:\n",
    "        print(' [*] Failed to find a checkpoint')\n",
    "        return False, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence and attribute risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_nearest_neighbor_distance(candidates, references, flag='noflag'):\n",
    "    \"\"\"Measures distance from candidate set to a reference set.\n",
    "    NOTE: This is not symmetric!\n",
    "\n",
    "    For each element in candidate set, find distance to nearest neighbor in\n",
    "    reference set. Return the average of these distances.\n",
    "\n",
    "    Args:\n",
    "      candidates: Numpy array of candidate points. (num_points x point_dim)\n",
    "      references: Numpy array of reference points. (num_points x point_dim)\n",
    "\n",
    "    Returns:\n",
    "      avg_dist: Float, average over distances.\n",
    "      distances: List of distances to nearest neighbor for each candidate.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for i in range(candidates.shape[0]):\n",
    "        c_i = tf.gather(candidates, [i])\n",
    "        distances_from_i = tf.norm(c_i - references, axis=1)\n",
    "        d_from_i_reshaped = tf.reshape(distances_from_i, [1, -1])  # NEW\n",
    "\n",
    "        assert d_from_i_reshaped.shape.as_list() == [1, references.shape[0]]\n",
    "        distances_negative = -1.0 * d_from_i_reshaped\n",
    "        #distances_negative = -1.0 * distances_from_i  # OLD\n",
    "        smallest_dist_negative, _ = tf.nn.top_k(distances_negative, name=flag)\n",
    "        assert smallest_dist_negative.shape.as_list() == [1, 1]\n",
    "        smallest_dist = -1.0 * smallest_dist_negative[0]\n",
    "\n",
    "        distances.append(smallest_dist)\n",
    "\n",
    "    avg_dist = tf.reduce_mean(distances)\n",
    "    return avg_dist, distances\n",
    "\n",
    "\n",
    "def evaluate_presence_risk(train, test, sim, ball_radius=1e-2):\n",
    "    \"\"\"Assess privacy of simulations.\n",
    "\n",
    "    Compute True Pos., True Neg., False Pos., and False Neg. rates of\n",
    "    finding a neighbor in the simulations, for each of a subset of training\n",
    "    data and a subset of test data.\n",
    "\n",
    "    Args:\n",
    "      train: Numpy array of all training data.\n",
    "      test: Numpy array of all test data (smaller than train).\n",
    "      sim: Numpy array of simulations.\n",
    "      ball_radius: Float, distance from point to sim that qualifies as match.\n",
    "\n",
    "    Return:\n",
    "      sensitivity: Float of TP / (TP + FN).\n",
    "      precision: Float of TP / (TP + FP).\n",
    "    \"\"\"\n",
    "    # TODO: Sensitivity as a loss, rather than just be reported?\n",
    "    # TODO: Count nearest neighbors, rather than presence in epsilon-ball?\n",
    "    assert len(test) < len(train), 'test should be smaller than train'\n",
    "    num_samples = len(test)\n",
    "    compromised_records = train[:num_samples]\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "    # Count true positives and false negatives.\n",
    "    for i in compromised_records:\n",
    "        distances_from_i = norm(i - sim, axis=1)\n",
    "        has_neighbor = np.any(distances_from_i < ball_radius)\n",
    "        if has_neighbor:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    # Count false positives and true negatives.\n",
    "    for i in test:\n",
    "        distances_from_i = norm(i - sim, axis=1)\n",
    "        has_neighbor = np.any(distances_from_i < ball_radius)\n",
    "        if has_neighbor:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "\n",
    "    sensitivity = float(tp) / (tp + fn)\n",
    "    precision = float(tp + 1e-10) / (tp + fp + 1e-10)\n",
    "    false_positive_rate = float(fp) / (fp + tn)\n",
    "    return (sensitivity, precision, false_positive_rate, tp, fn, fp, tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Med data plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_marginals(raw_data_train, data, batch_size, step, g_, g_out, log_dir,\n",
    "        filename_tag=None, plot_sparse=False):\n",
    "    \"\"\"Plots all marginals, and computes MMDs between marginals of real and sim.\n",
    "\n",
    "    Args:\n",
    "      raw_data_train: Numpy array, un-standardized data. \n",
    "      data: Numpy array, standardized data. \n",
    "      batch_size: Int, batch_size for MMD computation. \n",
    "      step: Int, used for logging. \n",
    "      g_: Numpy array, standardized simulation. \n",
    "      g_out: Numpy array, un-standardized simulation. \n",
    "      log_dir: String, path where plots are saved.\n",
    "      filename_tag: String, used for naming plot.\n",
    "      plot_spares: Bool, used to toggle labels on plots.\n",
    "    \"\"\"\n",
    "    random_batch_data = np.array(\n",
    "        [data[i] for i in np.random.choice(len(data), batch_size)])\n",
    "    random_batch_gen = np.array(\n",
    "        [g_[i] for i in np.random.choice(len(g_), batch_size)])\n",
    "    num_cols = raw_data_train.shape[1]\n",
    "    sq_dim = int(np.ceil(np.sqrt(num_cols)))\n",
    "    fig, axs = plt.subplots(sq_dim, sq_dim, figsize=(30, 30))\n",
    "    if not plot_sparse:\n",
    "        fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
    "        fig.suptitle('Marginals, it{}'.format(step))\n",
    "    else:\n",
    "        fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "    axs = axs.ravel()\n",
    "    bins = 40\n",
    "    for i in range(num_cols):\n",
    "        # For each marginal, compute mmd between normalized data and\n",
    "        # normalized simulations.\n",
    "        mmd_i_data_gen, _ = compute_mmd(\n",
    "            random_batch_data[:, i], random_batch_gen[:, i], use_tf=False)\n",
    "        # For each marginal, plot unnormalized data and unnormalized simulations.\n",
    "        plot_d = raw_data_train[:, i]\n",
    "        plot_g = g_out[:, i]\n",
    "        axs[i].hist(plot_d, density=True, alpha=0.3, label='d', bins=bins)\n",
    "        axs[i].hist(plot_g, density=True, alpha=0.3, label='g', bins=bins)\n",
    "        if not plot_sparse:\n",
    "            axs[i].set_xlabel('mmd = {:.3f}'.format(mmd_i_data_gen))\n",
    "            axs[i].legend()\n",
    "        else:\n",
    "            axs[i].tick_params(axis='both', which='both', bottom='off', top='off',\n",
    "                labelbottom='off', right='off', left='off', labelleft='off')\n",
    "    for i in range(num_cols, sq_dim ** 2):\n",
    "        axs[i].axis('off')\n",
    "    if filename_tag:\n",
    "        filename = 'plot_marginals_{}.png'.format(filename_tag)\n",
    "    else:\n",
    "        filename = 'plot_marginals_{}.png'.format(step)\n",
    "    plt.savefig(os.path.join(log_dir, filename))\n",
    "    plt.close('all')\n",
    "    \n",
    "    if jupyter_verbose:\n",
    "        display(Image(filename=os.path.join(log_dir, filename)))\n",
    "\n",
    "\n",
    "def plot_correlations(raw_data_train, step, g_out, log_dir):\n",
    "    num_cols = raw_data_train.shape[1]\n",
    "    corr_coefs_data = np.zeros((num_cols, num_cols))\n",
    "    corr_coefs_gens = np.zeros((num_cols, num_cols))\n",
    "    for i in range(num_cols):\n",
    "        for j in range(num_cols):\n",
    "            if j > i:\n",
    "                corr_coefs_data[i][j], _ = pearsonr(\n",
    "                        raw_data_train[:, i], raw_data_train[:, j])\n",
    "                corr_coefs_gens[i][j], _ = pearsonr(\n",
    "                        g_out[:, i], g_out[:, j])\n",
    "    coefs_d = corr_coefs_data.flatten()\n",
    "    coefs_g = corr_coefs_gens.flatten()\n",
    "    coefs_d = coefs_d[coefs_d != 0]\n",
    "    coefs_g = coefs_g[coefs_g != 0]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(coefs_d, coefs_g)\n",
    "    ax.plot(ax.get_xlim(), ax.get_ylim(), ls='-')\n",
    "    #ax.set_xlabel('Correlation data')\n",
    "    #ax.set_ylabel('Correlation gens')\n",
    "    filepath = os.path.join(\n",
    "        log_dir,'plot_correlations_{}.png'.format(step))\n",
    "    plt.savefig(filepath)\n",
    "    plt.close('all')\n",
    "    \n",
    "    if jupyter_verbose:\n",
    "        display(Image(filename=filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network helper function for DP-SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network_parameters(z_dim, default_gradient_l2norm_bound,\n",
    "                            depth, width, out_dim):\n",
    "    network_parameters = NetworkParameters()\n",
    "    network_parameters.input_size = z_dim\n",
    "    network_parameters.default_gradient_l2norm_bound = \\\n",
    "        default_gradient_l2norm_bound\n",
    "    for i in range(depth):\n",
    "        hidden = LayerParameters()\n",
    "        hidden.name = \"hidden%d\" % i\n",
    "        hidden.num_units = width\n",
    "        hidden.relu = True\n",
    "        hidden.with_bias = False\n",
    "        hidden.trainable = True\n",
    "        network_parameters.layer_parameters.append(hidden)\n",
    "\n",
    "    gen = LayerParameters()\n",
    "    gen.name = 'gen'\n",
    "    gen.num_units = out_dim \n",
    "    gen.relu = False\n",
    "    gen.with_bias = False\n",
    "    network_parameters.layer_parameters.append(gen)\n",
    "    return network_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TF graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_cmd_gan(batch_size, gen_num, data_num, data_test_num, out_dim,\n",
    "                        z_dim, cmd_span_const,\n",
    "                        moment_sensitivities,\n",
    "                        quantile_sensitivities,\n",
    "                        bounds):\n",
    "    \"\"\"Builds model for Central Moment Discrepancy as adversary.\"\"\"\n",
    "\n",
    "    # Placeholders to precompute avg distance from data_test to data.\n",
    "    x_precompute = tf.placeholder(\n",
    "        tf.float32, [data_test_num, out_dim], name='x_precompute')\n",
    "    x_test_precompute = tf.placeholder(\n",
    "        tf.float32, [data_test_num, out_dim], name='x_test_precompute')\n",
    "    avg_dist_x_to_x_test_precomputed, distances_xt_xp = \\\n",
    "        avg_nearest_neighbor_distance(x_precompute, x_test_precompute)\n",
    "\n",
    "    # Placeholders for regular training.\n",
    "    x = tf.placeholder(tf.float32, [batch_size, out_dim], name='x')\n",
    "    z = tf.placeholder(tf.float32, [gen_num, z_dim], name='z')\n",
    "    z_readonly = tf.placeholder(tf.float32, [data_num, z_dim], name='z_readonly')\n",
    "    x_test = tf.placeholder(tf.float32, [batch_size, out_dim], name='x_test')\n",
    "\n",
    "    avg_dist_x_to_x_test = tf.placeholder(\n",
    "        tf.float32, shape=(), name='avg_dist_x_to_x_test')\n",
    "    prog_cmd_coefs = tf.placeholder(\n",
    "        tf.float32, shape=(k_moments), name='prog_cmd_coefs')\n",
    "    mmd_to_cmd_indicator = tf.placeholder(\n",
    "        tf.float32, shape=(), name='mmd_to_cmd_indicator')\n",
    "\n",
    "    # Update learning rate.\n",
    "    lr = tf.Variable(learning_rate, name='lr', trainable=False)\n",
    "    lr_update = tf.assign(lr, tf.maximum(lr * 0.8, lr_minimum),\n",
    "                          name='lr_update')\n",
    "\n",
    "    ########################\n",
    "    # Generator output.\n",
    "    use_generator_v2 = 1\n",
    "    if use_generator_v2:\n",
    "        # Dense network using Google Research code from Github.\n",
    "        # https://github.com/tensorflow/models/blob/master/research/\n",
    "        #   differential_privacy/dp_sgd/dp_optimizer/utils.py\n",
    "        network_parameters = load_network_parameters(\n",
    "            z_dim, default_gradient_l2norm_bound, depth, width, out_dim)\n",
    "        with tf.variable_scope('generator_v2') as scope:\n",
    "            g = generator_v2(z, network_parameters)\n",
    "            g_readonly = generator_v2(z_readonly, network_parameters)\n",
    "    else:\n",
    "        g, g_vars = generator(\n",
    "            z, width=width, depth=depth, activation=activation, out_dim=out_dim,\n",
    "            bounds=bounds)\n",
    "        g_readonly, _ = generator(\n",
    "            z_readonly, width=width, depth=depth, activation=activation,\n",
    "            out_dim=out_dim, reuse=True, bounds=bounds)\n",
    "\n",
    "        \n",
    "    ########################\n",
    "    # Autoencoder output.\n",
    "    h_out, ae_out, enc_vars, dec_vars = autoencoder(tf.concat([x, g], 0),\n",
    "    width=width, depth=depth, activation=activation, z_dim=z_dim, reuse=False)\n",
    "    enc_x, enc_g = tf.split(h_out, [batch_size, gen_num])\n",
    "    ae_x, ae_g = tf.split(ae_out, [batch_size, gen_num])\n",
    "    \n",
    "        \n",
    "    #######################  \n",
    "    # Moment discrepancies.\n",
    "    on_encodings = 0\n",
    "    if on_encodings:\n",
    "        arr1 = enc_x\n",
    "        arr2 = enc_g\n",
    "    else:\n",
    "        arr1 = x\n",
    "        arr2 = g\n",
    "        \n",
    "    mmd = compute_mmd(\n",
    "        arr1, arr2, use_tf=True, slim_output=True, sigma_list=[0.1, 0.5, 1.0, 2.0])  # Added sigma_list.\n",
    "    kmmd = compute_kmmd(\n",
    "        arr1, arr2, k_moments=k_moments, use_tf=True,\n",
    "        slim_output=True, sigma_list=[0.1, 0.5, 1.0, 2.0])\n",
    "    cmd_k, cmd_k_terms = compute_cmd(\n",
    "        arr1, arr2, k_moments=k_moments, use_tf=True,\n",
    "        cmd_span_const=cmd_span_const, return_terms=True, taylor_weight=do_cmd_taylor_weights)\n",
    "    ncmd_k = compute_noncentral_moment_discrepancy(\n",
    "        arr1, arr2, k_moments=k_moments, use_tf=True,\n",
    "        cmd_span_const=cmd_span_const)\n",
    "    _, ncmd_k_terms = compute_noncentral_moment_discrepancy(\n",
    "        arr1, arr2, k_moments=k_moments, use_tf=True,\n",
    "        return_terms=True, cmd_span_const=1)\n",
    "    jmd_k = compute_joint_moment_discrepancy(\n",
    "        arr1, arr2, k_moments=k_moments, use_tf=True,\n",
    "        cmd_span_const=cmd_span_const)\n",
    "\n",
    "    \n",
    "    #######################\n",
    "    # Losses.\n",
    "\n",
    "    # Generator loss.\n",
    "    if cmd_variation == 'onetime_noisy':\n",
    "        # NoncentralMD on one-time-noised empirical data moments.\n",
    "        batch_id = tf.placeholder(tf.int32, shape=(), name='batch_id')\n",
    "        fbo_noisy_moments = tf.placeholder(\n",
    "            tf.float32, [None, k_moments, out_dim], name='fbo_noisy_moments')\n",
    "        g_loss = compute_noncentral_moment_discrepancy(\n",
    "            x, g, k_moments=k_moments, use_tf=True,\n",
    "            cmd_span_const=cmd_span_const, batch_id=batch_id,\n",
    "            fbo_noisy_moments=fbo_noisy_moments)\n",
    "        \n",
    "        eps = None\n",
    "        delta = None\n",
    "        fbo_noisy_jmoments = None\n",
    "        noisy_quantiles = None\n",
    "        priv_accountant = None\n",
    "        \n",
    "    elif cmd_variation == 'onetime_noisy_joint':\n",
    "        # Joint MD on one-time-noised empirical data moments.\n",
    "        batch_id = tf.placeholder(tf.int32, shape=(), name='batch_id')\n",
    "        fbo_noisy_moments = tf.placeholder(\n",
    "            tf.float32, [None, k_moments, out_dim], name='fbo_noisy_moments')\n",
    "        fbo_noisy_jmoments = tf.placeholder(\n",
    "            tf.float32, [None, k_moments, 1], name='fbo_noisy_jmoments')\n",
    "        num_quantiles = quantile_sensitivities.shape[-1]\n",
    "        noisy_quantiles = tf.placeholder(\n",
    "            tf.float32, [out_dim, num_quantiles], name='noisy_quantiles')\n",
    "        # Noncentral moment discrepancy.\n",
    "        ncmd_k = compute_noncentral_moment_discrepancy(\n",
    "            x, g, k_moments=k_moments, use_tf=True,\n",
    "            cmd_span_const=cmd_span_const, batch_id=batch_id,\n",
    "            fbo_noisy_moments=fbo_noisy_moments)\n",
    "        # Joint moment discrepancy.\n",
    "        jmd_k = compute_joint_moment_discrepancy(\n",
    "            x, g, k_moments=k_moments, use_tf=True,\n",
    "            cmd_span_const=cmd_span_const, batch_id=batch_id,\n",
    "            fbo_noisy_jmoments=fbo_noisy_jmoments)\n",
    "        # Combine the noncentral and joint discrepancies.\n",
    "        g_loss = 1 * ncmd_k + .1 * jmd_k\n",
    "\n",
    "        eps = None\n",
    "        delta = None\n",
    "        priv_accountant = None\n",
    "        \n",
    "    elif cmd_variation == 'dp_sgd':\n",
    "        eps = None\n",
    "        delta = None\n",
    "        priv_accountant = None\n",
    "        batch_id = None\n",
    "        fbo_noisy_moments = None\n",
    "        fbo_noisy_jmoments = None\n",
    "        noisy_quantiles = None\n",
    "        g_loss = mmd  # [mmd, cmd_k, ncmd_k]\n",
    "    \n",
    "    elif cmd_variation in ['mmd', 'kmmd', 'cmd', 'ncmd', 'ncmd_jmd']:\n",
    "        eps = None\n",
    "        delta = None\n",
    "        priv_accountant = None\n",
    "        batch_id = None\n",
    "        fbo_noisy_moments = None\n",
    "        fbo_noisy_jmoments = None\n",
    "        noisy_quantiles = None\n",
    "        if cmd_variation == 'mmd':\n",
    "            g_loss = mmd\n",
    "        elif cmd_variation == 'kmmd':\n",
    "            g_loss = kmmd\n",
    "        elif cmd_variation == 'cmd':\n",
    "            g_loss = cmd_k\n",
    "        elif cmd_variation == 'ncmd':\n",
    "            g_loss = ncmd_k\n",
    "        elif cmd_variation == 'ncmd_jmd':\n",
    "            g_loss = ncmd_k + jmd_k\n",
    "    \n",
    "    else:\n",
    "        sys.exit('Ensure valid cmd_variation.')\n",
    "\n",
    "    # Autoencoder loss.\n",
    "    ae_loss = tf.reduce_mean(tf.square(ae_x - x))\n",
    "    d_loss = ae_loss - 0.1 * g_loss\n",
    "\n",
    "        \n",
    "    ######################\n",
    "    # Optimization ops.\n",
    "\n",
    "    # DP-SGD optim nodes.\n",
    "    if cmd_variation == 'dp_sgd':\n",
    "        # Begin: Differentially private SGD.\n",
    "        # In classification example on Github, authors take average of cross-\n",
    "        # entropy loss across batch, claiming the \"actual cost is the average\n",
    "        # across the examples\". Here, the CMD is already an expectation over\n",
    "        # the samples, so it's unclear whether it should be scaled.\n",
    "        # TODO: Should the g_loss be scaled down by batch_size?\n",
    "\n",
    "        # Define effective training size, given fixed batches.\n",
    "        num_training = (data_num // batch_size) * batch_size\n",
    "\n",
    "        # Instantiate the accountant.\n",
    "        priv_accountant = GaussianMomentsAccountant(num_training)\n",
    "\n",
    "        # Define sigma.\n",
    "        sigma = sgd_sigma\n",
    "        \n",
    "        # Instantiate the sanitizer.\n",
    "        # TODO: Should the bound be scaled down by batch size?\n",
    "        gaussian_sanitizer = AmortizedGaussianSanitizer(\n",
    "            priv_accountant,\n",
    "            [default_gradient_l2norm_bound / batch_size, True])  # / batch_size?\n",
    "\n",
    "        # Setting clip options for each var. For now, this does nothing, as all\n",
    "        # vars take default option.\n",
    "        #for var in training_params:\n",
    "        #    if \"gradient_l2norm_bound\" in training_params[var]:\n",
    "        #        l2bound = training_params[var][\"gradient_l2norm_bound\"] / batch_size\n",
    "        #        gaussian_sanitizer.set_option(var, sanitizer.ClipOption(l2bound,\n",
    "        #                                                                True))\n",
    "\n",
    "        # Constants for optimization step.\n",
    "        #lr = tf.placeholder(tf.float32)\n",
    "        eps = tf.placeholder(tf.float32)\n",
    "        delta = tf.placeholder(tf.float32)\n",
    "\n",
    "        # Define optimization node.\n",
    "        g_optim = DPGradientDescentOptimizer(\n",
    "            lr,\n",
    "            [eps, delta],\n",
    "            gaussian_sanitizer,\n",
    "            sigma=sigma,\n",
    "            batches_per_lot=1).minimize(g_loss)\n",
    "        \n",
    "        # TODO: DP-SGD version of autoencoder optim.\n",
    "        # TODO: Figure whether dp-sgd version needs to have its\n",
    "        #       var_list restricted to generator vars.\n",
    "\n",
    "    # Moment discrepancy optim nodes.\n",
    "    else:\n",
    "        if optimizer == 'adagrad':\n",
    "            g_opt = tf.train.AdagradOptimizer(lr)\n",
    "            d_opt = tf.train.AdagradOptimizer(lr)\n",
    "        elif optimizer == 'adam':\n",
    "            g_opt = tf.train.AdamOptimizer(lr)\n",
    "            d_opt = tf.train.AdamOptimizer(lr)\n",
    "        elif optimizer == 'rmsprop':\n",
    "            g_opt = tf.train.RMSPropOptimizer(lr)\n",
    "            d_opt = tf.train.RMSPropOptimizer(lr)\n",
    "        elif optimizer == 'adadelta':\n",
    "            g_opt = tf.train.AdadeltaOptimizer(lr)\n",
    "            d_opt = tf.train.AdadeltaOptimizer(lr)\n",
    "        else:\n",
    "            g_opt = tf.train.GradientDescentOptimizer(lr)\n",
    "            d_opt = tf.train.GradientDescentOptimizer(lr)\n",
    "            \n",
    "        d_optim = d_opt.minimize(d_loss, var_list=enc_vars+dec_vars)\n",
    "            \n",
    "        g_vars = [v for v in tf.trainable_variables() if 'generator' in v.name]\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)  # layers.batch_normalization\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            clip = 1\n",
    "            if clip:\n",
    "                #g_vars = tf.trainable_variables()  # NEW to v2\n",
    "                g_grads_, g_vars_ = zip(*g_opt.compute_gradients(g_loss, var_list=g_vars))\n",
    "                g_grads_clipped_ = tuple(\n",
    "                    [tf.clip_by_value(grad, -0.01, 0.01) for grad in g_grads_])\n",
    "                g_optim = g_opt.apply_gradients(zip(g_grads_clipped_, g_vars_))\n",
    "            else:\n",
    "                #g_vars = tf.trainable_variables()  # NEW to v2\n",
    "                g_optim = g_opt.minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "\n",
    "    \n",
    "    ###########################\n",
    "    # Diagnostics and summary.\n",
    "    \n",
    "    # Get diagnostics. Compare distances between data, heldouts, and gen.\n",
    "    avg_dist_g_to_x, distances_g_x = avg_nearest_neighbor_distance(g, x)\n",
    "    avg_dist_x_to_g, distances_x_g = avg_nearest_neighbor_distance(x, g)\n",
    "    avg_dist_x_test_to_g, distances_xt_g = avg_nearest_neighbor_distance(x_test, g)\n",
    "    loss1 = avg_dist_x_to_x_test - avg_dist_x_to_g\n",
    "    loss2 = avg_dist_x_test_to_g - avg_dist_x_to_g\n",
    "    cmd = cmd_k  # For reporting, define cmd as cmd_k.\n",
    "\n",
    "    # Define summary op for reporting.\n",
    "    summary_op = tf.summary.merge([\n",
    "    tf.summary.scalar(\"loss/g_loss\", g_loss),\n",
    "    tf.summary.scalar(\"loss/g_loss\", d_loss),\n",
    "    tf.summary.scalar(\"loss/g_loss\", ae_loss),\n",
    "    tf.summary.scalar(\"loss/loss1\", loss1),\n",
    "    tf.summary.scalar(\"loss/loss2\", loss2),\n",
    "    tf.summary.scalar(\"loss/mmd\", mmd),\n",
    "    tf.summary.scalar(\"loss/cmd\", cmd),\n",
    "    tf.summary.scalar(\"loss/ncmd_k\", ncmd_k),\n",
    "    tf.summary.scalar(\"loss/jmd_k\", jmd_k),\n",
    "    tf.summary.scalar(\"misc/lr\", lr),\n",
    "    ])\n",
    "\n",
    "    return (x, z, z_readonly, x_test, x_precompute, x_test_precompute,\n",
    "            avg_dist_x_to_x_test, avg_dist_x_to_x_test_precomputed,\n",
    "            distances_xt_xp, prog_cmd_coefs, mmd_to_cmd_indicator, cmd_k_terms,\n",
    "            ncmd_k_terms, g, g_readonly, mmd, kmmd, cmd, loss1, loss2, lr_update, lr,\n",
    "            g_optim, d_optim, d_loss, ae_loss, summary_op, batch_id, fbo_noisy_moments,\n",
    "            fbo_noisy_jmoments, noisy_quantiles, eps, delta, priv_accountant)\n",
    "\n",
    "\n",
    "def add_nongraph_summary_items(summary_writer, step, dict_to_add):\n",
    "    \"\"\"Adds to list of summary items during logging.\"\"\"\n",
    "    for k, v in dict_to_add.items():\n",
    "        summ = tf.Summary()\n",
    "        summ.value.add(tag=k, simple_value=v)\n",
    "        summary_writer.add_summary(summ, step)\n",
    "    summary_writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare logging, checkpoint, and plotting directories.\n",
    "log_dir, checkpoint_dir, plot_dir, g_out_dir = prepare_dirs(load_existing)\n",
    "save_tag = str(args)\n",
    "with open(os.path.join(log_dir, 'save_tag.txt'), 'w') as save_tag_file:\n",
    "    save_tag_file.write(save_tag)\n",
    "print('Save tag: {}'.format(save_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and prep dirs.\n",
    "(data, \n",
    " data_test, \n",
    " data_num, \n",
    " data_test_num, \n",
    " out_dim, \n",
    " data_raw_mean,\n",
    " data_raw_std,\n",
    " clip) = load_normed_data(data_num_init, percent_train, log_dir,\n",
    "                          clip_unnormed=clip_unnormed,\n",
    "                          data_file=data_file)\n",
    "data_dim = data.shape[1]\n",
    "normed_moments_data = compute_moments(data, k_moments=k_moments+1)\n",
    "normed_moments_data_test = compute_moments(data_test, k_moments=k_moments+1)\n",
    "nmd_zero_indices = np.argwhere(\n",
    "    norm(np.array(normed_moments_data), axis=1) < 0.1)\n",
    "\n",
    "# Compute baseline statistics on moments for data set.\n",
    "#print_baseline_moment_stats(data, data_raw_mean, data_raw_std, k_moments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sensitivities for moments.\n",
    "fixed_batches = make_fixed_batches(data, batch_size)\n",
    "\n",
    "(moment_sensitivities,\n",
    " jmoment_sensitivities,\n",
    " global_quantile_values,\n",
    " quantile_sensitivities) = compute_sensitivities(data, batch_size, k_moments, clip=clip)\n",
    "print('Global quantile values:')\n",
    "print(global_quantile_values)\n",
    "\n",
    "print('\\n\\nData size: {}, Batch size: {}, Num batches: {}, '\n",
    "      'Effective data size: {}\\n\\n'.format(\n",
    "           len(data), batch_size, len(fixed_batches),\n",
    "           batch_size * len(fixed_batches)))\n",
    "\n",
    "# Allocation can apply more of privacy budget to certain moments.\n",
    "# e.g. allocation = [1, 1, 5] applies 5/7 of the eps budget to the third moment.\n",
    "# More budget leads to lower noise applied to that moment.\n",
    "allocation = [1] * k_moments\n",
    "\n",
    "# Add onetime noise to MOMENT in each batch, according to \n",
    "# moment_sensitivities.\n",
    "fixed_batches_onetime_noisy_moments = \\\n",
    "    make_fbo_noisy_moments(\n",
    "        fixed_batches, moment_sensitivities, k_moments,\n",
    "        laplace_eps, allocation=allocation)\n",
    "\n",
    "# Add onetime noise to JOINT MOMENT in each batch, according to \n",
    "# fixed_batches_jmoment_sensitivities.\n",
    "allocation = [1] * k_moments\n",
    "fixed_batches_onetime_noisy_jmoments = \\\n",
    "    make_fbo_noisy_jmoments(\n",
    "        fixed_batches, jmoment_sensitivities, k_moments,\n",
    "        laplace_eps, allocation=allocation)\n",
    "fixed_batches_onetime_noisy_jmoments = np.expand_dims(\n",
    "    fixed_batches_onetime_noisy_jmoments, axis=2)\n",
    "assert (len(fixed_batches_onetime_noisy_moments.shape) ==\n",
    "        len(fixed_batches_onetime_noisy_jmoments.shape) == 3), (\n",
    "            'fbo inputs must be 3d tensors')\n",
    "\n",
    "# Add onetime noise to quantiles in each batch, according to \n",
    "# quantile_sensitivities.\n",
    "onetime_noisy_quantiles = \\\n",
    "    make_noisy_quantiles(\n",
    "        global_quantile_values,\n",
    "        quantile_sensitivities,\n",
    "        laplace_eps)\n",
    "\n",
    "# Get compact interval bounds for CMD computations.\n",
    "data_raw = data * data_raw_std + data_raw_mean\n",
    "cmd_a_raw = np.min(data_raw)\n",
    "cmd_b_raw = np.max(data_raw)\n",
    "cmd_span_const = 1.0 / np.max(pdist(data))  # TODO: Is it a problem that this is on [0, 1]?\n",
    "#print('OVERWROTE SPAN CONSTANT to 1.')\n",
    "#cmd_span_const = 1.\n",
    "cmd_a = np.min(data)\n",
    "cmd_b = np.max(data)\n",
    "print('cmd_span_const: {:.2f}'.format(cmd_span_const))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data and build model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data set used for training.\n",
    "data_train_unnormed = data * data_raw_std + data_raw_mean\n",
    "data_test_unnormed = data_test * data_raw_std + data_raw_mean\n",
    "np.save(os.path.join(log_dir, 'data_train.npy'), data_train_unnormed)\n",
    "np.save(os.path.join(log_dir, 'data_test.npy'), data_test_unnormed)\n",
    "\n",
    "# Save file for outputs in txt form. Also saved later as npy.\n",
    "g_out_file = os.path.join(g_out_dir, 'g_out.txt')\n",
    "if os.path.isfile(g_out_file):\n",
    "    os.remove(g_out_file)\n",
    "\n",
    "# build_all()\n",
    "# Build model.\n",
    "(x, z, z_readonly, x_test, x_precompute, x_test_precompute,\n",
    " avg_dist_x_to_x_test, avg_dist_x_to_x_test_precomputed, distances_xt_xp,\n",
    " prog_cmd_coefs, mmd_to_cmd_indicator, cmd_k_terms, ncmd_k_terms, g, g_readonly,\n",
    " mmd, kmmd, cmd, loss1, loss2, lr_update, lr, g_optim, d_optim, d_loss, ae_loss,\n",
    " summary_op, batch_id, fbo_noisy_moments, fbo_noisy_jmoments, noisy_quantiles,\n",
    " eps, delta, priv_accountant) = \\\n",
    "     build_model_cmd_gan(batch_size, gen_num, data_num, data_test_num,\n",
    "                         out_dim, z_dim, cmd_span_const,\n",
    "                         moment_sensitivities,\n",
    "                         quantile_sensitivities, bounds=[cmd_a, cmd_b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Start session.\n",
    "################\n",
    "init_op = tf.global_variables_initializer()\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess_config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    if cmd_variation == 'dp_sgd':\n",
    "        # We need to maintain the intialization sequence.\n",
    "        for v in tf.trainable_variables():\n",
    "            sess.run(tf.variables_initializer([v]))\n",
    "\n",
    "\n",
    "    saver, summary_writer = prepare_logging(log_dir, checkpoint_dir, sess)\n",
    "\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # load_existing_model().\n",
    "    if load_existing:\n",
    "        could_load, checkpoint_counter = load_checkpoint(\n",
    "            saver, sess, checkpoint_dir)\n",
    "        if could_load:\n",
    "            load_step = checkpoint_counter\n",
    "            print(' [*] Load SUCCESS')\n",
    "        else:\n",
    "            print(' [!] Load failed...')\n",
    "    else:\n",
    "        load_step = 0\n",
    "\n",
    "    # Once, compute average distance from heldout data to training data.\n",
    "    avg_dist_x_to_x_test_precomputed_, _ = sess.run(\n",
    "        [avg_dist_x_to_x_test_precomputed, distances_xt_xp],\n",
    "        {x_precompute: data[:len(data_test)],\n",
    "         x_test_precompute: data_test})\n",
    "\n",
    "    # Containers to hold empirical and relative errors of moments.\n",
    "    empirical_moments_gens = np.zeros(\n",
    "        ((max_step - 0) // log_step, k_moments+1, data_dim))\n",
    "    relative_error_of_moments = np.zeros(\n",
    "        ((max_step - 0) // log_step, k_moments+1))\n",
    "    reom = relative_error_of_moments\n",
    "\n",
    "\n",
    "    #########\n",
    "    # train()\n",
    "    #########\n",
    "    start_time = time()\n",
    "    for step in range(load_step, max_step):\n",
    "\n",
    "        # Set up inputs for all models.\n",
    "        # OPTION 1: Random batch selection.\n",
    "        # OPTION 2: Fixed batch selection.\n",
    "        batch_selection_option = 2\n",
    "        if batch_selection_option == 1:\n",
    "            random_batch_data = np.array(\n",
    "                [data[d] for d in np.random.choice(len(data), batch_size)])\n",
    "            _batch_id = None\n",
    "        elif batch_selection_option == 2:\n",
    "            _epoch, _batch_id = np.divmod(step, len(fixed_batches))\n",
    "            if _batch_id == 0 and _epoch % 10 == 0:\n",
    "                print(' {}'.format(_epoch))\n",
    "            random_batch_data = fixed_batches[_batch_id]\n",
    "\n",
    "        # Fetch test data and z.\n",
    "        random_batch_data_test = np.array(\n",
    "            [data_test[d] for d in np.random.choice(\n",
    "                len(data_test), batch_size)])\n",
    "        random_batch_z = get_random_z(gen_num, z_dim)\n",
    "\n",
    "        # Update shared dict for chosen model.\n",
    "        #if cmd_variation in ['', None]:\n",
    "        #    feed_dict = {\n",
    "        #        x: random_batch_data,\n",
    "        #        z: random_batch_z,\n",
    "        #        x_test: random_batch_data_test,\n",
    "        #        avg_dist_x_to_x_test: avg_dist_x_to_x_test_precomputed_,\n",
    "        #        batch_id: _batch_id}\n",
    "        if cmd_variation == 'dp_sgd':\n",
    "            feed_dict = {\n",
    "                x: random_batch_data,\n",
    "                z: random_batch_z,\n",
    "                x_test: random_batch_data_test,\n",
    "                avg_dist_x_to_x_test: avg_dist_x_to_x_test_precomputed_,\n",
    "                eps: sgd_eps,\n",
    "                delta: sgd_delta}\n",
    "        elif cmd_variation == 'onetime_noisy': \n",
    "            feed_dict = {\n",
    "                x: random_batch_data,\n",
    "                z: random_batch_z,\n",
    "                x_test: random_batch_data_test,\n",
    "                avg_dist_x_to_x_test: avg_dist_x_to_x_test_precomputed_,\n",
    "                batch_id: _batch_id,\n",
    "                fbo_noisy_moments: fixed_batches_onetime_noisy_moments}\n",
    "        elif cmd_variation == 'onetime_noisy_joint': \n",
    "            feed_dict = {\n",
    "                x: random_batch_data,\n",
    "                z: random_batch_z,\n",
    "                x_test: random_batch_data_test,\n",
    "                avg_dist_x_to_x_test: avg_dist_x_to_x_test_precomputed_,\n",
    "                batch_id: _batch_id,\n",
    "                fbo_noisy_moments: fixed_batches_onetime_noisy_moments,\n",
    "                fbo_noisy_jmoments: fixed_batches_onetime_noisy_jmoments}\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                x: random_batch_data,\n",
    "                z: random_batch_z,\n",
    "                x_test: random_batch_data_test,\n",
    "                avg_dist_x_to_x_test: avg_dist_x_to_x_test_precomputed_}\n",
    "\n",
    "        \n",
    "        # Run optimization step.\n",
    "        #sess.run([d_optim, g_optim], feed_dict)\n",
    "        sess.run([g_optim], feed_dict)\n",
    "\n",
    "        # Occasionally update learning rate.\n",
    "        if step % lr_update_step == lr_update_step - 1:\n",
    "            _, lr_ = sess.run([lr_update, lr])\n",
    "            print('Updated learning rate to {}'.format(lr_))\n",
    "\n",
    "\n",
    "        ###########\n",
    "        # logging()\n",
    "        ###########\n",
    "        # Occasionally log/plot results.\n",
    "        if step % log_step == 0 and step > 0:\n",
    "            print('\\nIter: {}'.format(step))\n",
    "\n",
    "            if cmd_variation == 'dp_sgd':\n",
    "                # Report privacy loss.\n",
    "                spent_eps_deltas = priv_accountant.get_privacy_spent(\n",
    "                    sess, target_eps=sgd_target_eps)\n",
    "                for spent_eps, spent_delta in spent_eps_deltas:\n",
    "                    print('  spent privacy: eps {:.4f} delta {:.5g}'.format(\n",
    "                          spent_eps, spent_delta))\n",
    "\n",
    "            # Read off from graph.\n",
    "            (cmd_, cmd_k_terms_, ncmd_k_terms_,\n",
    "             mmd_, kmmd_, d_loss_, ae_loss_,\n",
    "             loss1_, loss2_, \n",
    "             summary_result) = sess.run([cmd, cmd_k_terms, ncmd_k_terms,\n",
    "                                         mmd, kmmd, d_loss, ae_loss,\n",
    "                                         loss1, loss2,\n",
    "                                         summary_op],\n",
    "                                        feed_dict)\n",
    "            g_readonly_ = sess.run(g_readonly,\n",
    "                                   {z_readonly: get_random_z(data_num,\n",
    "                                                             z_dim,\n",
    "                                                             for_training=False)})\n",
    "            g_batch_ = g_readonly_[np.random.randint(0, data_num, batch_size)]\n",
    "            g_full_ = g_readonly_\n",
    "            # TODO: Determine how much budget to put on joint moment.\n",
    "            if cmd_variation == 'onetime_noisy':\n",
    "                total_laplace_eps = laplace_eps\n",
    "            elif cmd_variation == 'onetime_noisy_joint':\n",
    "                total_laplace_eps = 2 * laplace_eps\n",
    "            else:\n",
    "                total_laplace_eps = laplace_eps\n",
    "            print(('  LAPLACE_EPS: {:.1f},\\n'\n",
    "                   'MMD: {:.4f}, kmmd: {:.4f}, cmd: {:.4f},\\n'\n",
    "                   'ncmd_k_terms: {}\\n'\n",
    "                   'd_loss: {:.1f}, ae_loss: {:.1f}, '\n",
    "                   'loss1: {:.4f}, loss2: {:.4f}').format(\n",
    "                       total_laplace_eps,\n",
    "                       mmd_, kmmd_, cmd_,\n",
    "                       ncmd_k_terms_,\n",
    "                       d_loss_, ae_loss_,\n",
    "                       loss1_, loss2_))\n",
    "\n",
    "\n",
    "            # Test joint moment discrepancy.\n",
    "            md_batch = compute_noncentral_moment_discrepancy(\n",
    "                random_batch_data, g_batch_, k_moments=k_moments,\n",
    "                cmd_span_const=cmd_span_const, batch_id=_batch_id,\n",
    "                fbo_noisy_moments=fixed_batches_onetime_noisy_moments)\n",
    "            jmd_batch = compute_joint_moment_discrepancy(\n",
    "                random_batch_data, g_batch_, k_moments=k_moments,\n",
    "                cmd_span_const=cmd_span_const, batch_id=_batch_id,\n",
    "                fbo_noisy_jmoments=fixed_batches_onetime_noisy_jmoments)\n",
    "            print('  MD_batch: {:.4f}'.format(md_batch))\n",
    "            print('  JMD_batch: {:.4f}'.format(jmd_batch))\n",
    "            \n",
    "            # Test kmmd.\n",
    "            kmmd_test = compute_kmmd(random_batch_data, g_batch_, k_moments=k_moments,\n",
    "                                     use_tf=False, slim_output=True)\n",
    "\n",
    "            # Diagnose NaNs.\n",
    "            #if np.isnan(mmd_):\n",
    "            if np.isnan(mmd_):\n",
    "                pdb.set_trace()\n",
    "\n",
    "            # Unormalize data and simulations for all logs and plots.\n",
    "            g_batch_unnormed = unnormalize(\n",
    "                g_batch_, data_raw_mean, data_raw_std)\n",
    "            g_full_unnormed = unnormalize(\n",
    "                g_full_, data_raw_mean, data_raw_std)\n",
    "            data_unnormed = unnormalize(\n",
    "                data, data_raw_mean, data_raw_std)\n",
    "            data_test_unnormed = unnormalize(\n",
    "                data_test, data_raw_mean, data_raw_std)\n",
    "\n",
    "            if extra_verbose:\n",
    "                # Compute disclosure risk.\n",
    "                (sensitivity, precision, false_positive_rate, tp, fn, fp,\n",
    "                 tn) = evaluate_presence_risk(\n",
    "                     data_unnormed, data_test_unnormed, g_full_unnormed)\n",
    "                     #ball_radius=avg_dist_x_to_x_test_precomputed_)\n",
    "                sens_minus_fpr = sensitivity - false_positive_rate\n",
    "                print('  Sens={:.4f}, Prec={:.4f}, Fpr: {:.4f}, '\n",
    "                      'tp: {}, fn: {}, fp: {}, tn: {}'.format(\n",
    "                          sensitivity, precision, false_positive_rate, tp, fn,\n",
    "                          fp, tn))\n",
    "\n",
    "                # Add presence disclosure stats to summaries.\n",
    "                summary_writer.add_summary(summary_result, step)\n",
    "                add_nongraph_summary_items(\n",
    "                    summary_writer, step,\n",
    "                    {'misc/sensitivity': sensitivity,\n",
    "                     'misc/false_positive_rate': false_positive_rate,\n",
    "                     'misc/sens_minus_fpr': sens_minus_fpr,\n",
    "                     'misc/precision': precision})\n",
    "\n",
    "            # Save checkpoint.\n",
    "            saver.save(\n",
    "                sess,\n",
    "                os.path.join(log_dir, 'checkpoints', model_type),\n",
    "                global_step=step)\n",
    "\n",
    "            # Save generated data to file.\n",
    "            np.save(os.path.join(g_out_dir, 'g_out_{}.npy'.format(step)),\n",
    "                    g_full_unnormed)\n",
    "            with open(g_out_file, 'a') as f:\n",
    "                f.write(str(g_full_unnormed) + '\\n')\n",
    "\n",
    "            # Print time performance.\n",
    "            if step % (10 * log_step) == 0 and step > 0:\n",
    "                elapsed_time = time() - start_time\n",
    "                time_per_iter = elapsed_time / step\n",
    "                total_est = elapsed_time / step * max_step\n",
    "                m, s = divmod(total_est, 60)\n",
    "                h, m = divmod(m, 60)\n",
    "                total_est_str = '{:.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)\n",
    "                print(('\\n  Time (s): {:.2f}, time/iter: {:.4f},'\n",
    "                       ' Total est.: {}').format(\n",
    "                            elapsed_time, time_per_iter, total_est_str))\n",
    "\n",
    "                print('  Save tag: {}\\n'.format(save_tag))\n",
    "\n",
    "\n",
    "            ############################\n",
    "            # PLOT data and simulations.\n",
    "            ############################\n",
    "            if out_dim == 1:\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.hist(data, density=True, bins=30, color='gray', alpha=0.3,\n",
    "                        label='data')\n",
    "                ax.hist(g_full_, density=True, bins=30, color='blue', alpha=0.2,\n",
    "                        label='g_full_readonly')\n",
    "\n",
    "                plt.legend()\n",
    "                filepath = os.path.join(plot_dir, '{}.png'.format(step))\n",
    "                plt.savefig(filepath)\n",
    "                plt.close(fig)\n",
    "                \n",
    "                if jupyter_verbose:\n",
    "                    display(Image(filename=filepath))\n",
    "            \n",
    "            elif out_dim == 2:\n",
    "                d_x = data_unnormed[:, 0]\n",
    "                d_y = data_unnormed[:, 1]\n",
    "                g_x = g_full_unnormed[:, 0]\n",
    "                g_y = g_full_unnormed[:, 1]\n",
    "\n",
    "                fig = plt.figure()\n",
    "                gs = GridSpec(4, 4)\n",
    "                ax_joint = fig.add_subplot(gs[1:4, 0:3])\n",
    "                ax_marg_x = fig.add_subplot(gs[0, 0:3], sharex=ax_joint)\n",
    "                ax_marg_y = fig.add_subplot(gs[1:4, 3], sharey=ax_joint)\n",
    "\n",
    "                ax_joint.scatter(*zip(*data_unnormed), color='gray',\n",
    "                                 alpha=0.2, label='data')\n",
    "                ax_joint.scatter(*zip(*g_full_unnormed), color='green',\n",
    "                                 alpha=0.2, label='sim')\n",
    "                bins_x = np.arange(np.min([np.min(d_x), np.min(g_x)]),\n",
    "                                   np.max([np.max(d_x), np.max(g_x)]), 0.2)\n",
    "                bins_y = np.arange(np.min([np.min(d_y), np.min(g_y)]),\n",
    "                                   np.max([np.max(d_y), np.max(g_y)]), 0.2)\n",
    "                #bins_y = np.arange(np.min(d_y), np.max(d_y), 0.2)\n",
    "                ax_marg_x.hist([d_x, g_x], bins=bins_x, alpha=0.2,\n",
    "                               color=['gray', 'green'], label=['data', 'gen'],\n",
    "                               density=True)\n",
    "                ax_marg_y.hist([d_y, g_y], bins=bins_y, alpha=0.2, \n",
    "                               color=['gray', 'green'], label=['data', 'gen'],\n",
    "                               density=True, orientation='horizontal')\n",
    "                ax_joint.legend()\n",
    "                ax_marg_x.legend()\n",
    "                ax_marg_y.legend()\n",
    "                plt.setp(ax_marg_x.get_xticklabels(), visible=False)\n",
    "                plt.setp(ax_marg_y.get_yticklabels(), visible=False)\n",
    "                plt.suptitle(tag)\n",
    "                filepath = os.path.join(plot_dir, '{}.png'.format(step))\n",
    "                plt.savefig(filepath)\n",
    "                plt.close()\n",
    "\n",
    "                if jupyter_verbose:\n",
    "                    display(Image(filename=filepath))\n",
    "\n",
    "                    \n",
    "            # Plot moment diagnostics.\n",
    "            if data_dim <= 2:\n",
    "                normed_moments_gens = compute_moments(\n",
    "                    g_full_, k_moments=k_moments+1)\n",
    "                empirical_moments_gens[step // log_step] = normed_moments_gens\n",
    "\n",
    "                # Define colormap used for plotting.\n",
    "                cmap = plt.cm.get_cmap('cool', k_moments+1)\n",
    "\n",
    "                if data_dim == 1:\n",
    "                    # Plot empirical moments throughout training.\n",
    "                    fig, (ax_data, ax_gens) = plt.subplots(2, 1)\n",
    "                    for i in range(k_moments+1):\n",
    "                        ax_data.axhline(y=normed_moments_data[i],\n",
    "                                        label='m{}'.format(i+1), c=cmap(i))\n",
    "                        ax_gens.plot(empirical_moments_gens[:step//log_step, i],\n",
    "                                     label='m{}'.format(i+1), c=cmap(i),\n",
    "                                     alpha=0.8)\n",
    "                    ax_data.set_ylim(min(normed_moments_data)-0.5,\n",
    "                                     max(normed_moments_data)+0.5)\n",
    "                    ax_gens.set_xlabel('Empirical moments, gens')\n",
    "                    ax_data.set_xlabel('Empirical moments, data')\n",
    "                    ax_gens.legend()\n",
    "                    ax_data.legend()\n",
    "                    plt.suptitle('{}, empirical moments, k={}'.format(\n",
    "                        tag, k_moments))\n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(os.path.join(\n",
    "                        plot_dir, 'empirical_moments.png'))\n",
    "                    plt.close(fig)\n",
    "\n",
    "                # Plot relative error of moments.\n",
    "                relative_error_of_moments_test = (\n",
    "                    norm(np.array(normed_moments_data_test) -\n",
    "                         np.array(normed_moments_data), axis=1) /\n",
    "                    norm(np.array(normed_moments_data), axis=1))\n",
    "                relative_error_of_moments_gens = (\n",
    "                    norm(np.array(normed_moments_gens) -\n",
    "                         np.array(normed_moments_data), axis=1) /\n",
    "                    norm(np.array(normed_moments_data), axis=1))\n",
    "\n",
    "                relative_error_of_moments_test[nmd_zero_indices] = 0.0\n",
    "                relative_error_of_moments_gens[nmd_zero_indices] = 0.0\n",
    "                reom[step // log_step] = relative_error_of_moments_gens\n",
    "\n",
    "                if extra_verbose:\n",
    "                    if data_dim <= 2:\n",
    "                        print('  Relative_error_of_moments_TEST: {}'.format(list(\n",
    "                            np.round(relative_error_of_moments_test, 2))))\n",
    "                        print('  Relative_error_of_moments_GENS: {}'.format(list(\n",
    "                            np.round(relative_error_of_moments_gens, 2))))\n",
    "\n",
    "                # For plotting, zero-out moments that are likely zero, so\n",
    "                # their relative values don't dominate the plot.\n",
    "                reom_trim_level = np.max(np.abs(reom[:, :k_moments]))\n",
    "                reom_trimmed = np.copy(reom)\n",
    "                reom_trimmed[\n",
    "                    np.where(reom_trimmed > reom_trim_level)] = \\\n",
    "                            2 * reom_trim_level\n",
    "                reom_trimmed[\n",
    "                    np.where(reom_trimmed < -reom_trim_level)] = \\\n",
    "                            -2 * reom_trim_level\n",
    "                fig, ax = plt.subplots()\n",
    "                for i in range(k_moments+1):\n",
    "                    ax.plot(reom[:step//log_step, i],\n",
    "                            label='m{}'.format(i+1), c=cmap(i))\n",
    "                #ax.set_ylim((-2 * reom_trim_level, 2 * reom_trim_level))\n",
    "                ax.set_ylim((-2, 2))\n",
    "                ax.legend()\n",
    "                plt.suptitle('{}, relative errors of moments, k={}'.format(\n",
    "                    tag, k_moments))\n",
    "                plt.savefig(os.path.join(\n",
    "                    plot_dir, 'reom.png'))\n",
    "                plt.close(fig)\n",
    "\n",
    "                # Print normed moments to console.\n",
    "                if extra_verbose:\n",
    "                    if data_dim == 1:\n",
    "                        print('    data_normed moments: {}'.format(\n",
    "                            normed_moments_data))\n",
    "                        print('    test_normed moments: {}'.format(\n",
    "                            normed_moments_data_test))\n",
    "                        print('    gens_normed moments: {}'.format(\n",
    "                            normed_moments_gens))\n",
    "            \n",
    "            # Plot yangmed data.\n",
    "            if 'yang' in data_file:\n",
    "                binary_cols = [17, 18, 19]\n",
    "                for i, row in enumerate(g_full_unnormed):\n",
    "                    for col in binary_cols:\n",
    "                        if row[col] < 0.5:\n",
    "                            row[col] = 0.0\n",
    "                        else:\n",
    "                            row[col] = 1.0\n",
    "                    g_full_unnormed[i] = row\n",
    "                plot_marginals(data_unnormed, data, batch_size, step, g_full_, g_full_unnormed, log_dir)\n",
    "                plot_correlations(data_unnormed, step, g_full_unnormed, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
