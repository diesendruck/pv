{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Points and Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.1 (default, Nov 28 2018, 11:55:14) \n",
      "[Clang 9.0.0 (clang-900.0.39.2)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Toggle which experiments to run.\n",
    "DO_EXPERIMENT_supportpoints = 0\n",
    "DO_EXPERIMENT_tensorflow = 0\n",
    "DO_EXPERIMENT_cdf2pdf = 0\n",
    "DO_EXPERIMENT_empirical_minima = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: \n",
    "## Empirical distance between support points Y and leave-one-out \\[LOO\\] support points (Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Computes max discrepancy of support points due to single-point omission.\n",
    "\n",
    "For data X, and support points Y:\n",
    "M(X, m) --> Y\n",
    "M(X', m) --> Y'\n",
    "Compute Pr(d(Y, Y') > eps) < delta\n",
    "\n",
    "let eps = (1-delta)th percentile of max_discrepancies\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import numpy as np\n",
    "import os\n",
    "import pdb\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = 10\n",
    "all_but_one = np.concatenate((10 * np.ones(1), np.random.normal(0, 1, m - 1)))  # \"All-but-one\" distribution\n",
    "\n",
    "def sample_gaussian(m):\n",
    "    mean, loc = 0, 1\n",
    "    if m == 1:\n",
    "        return np.random.normal(mean, loc, m)\n",
    "    else:\n",
    "        return np.concatenate((10 * np.ones(1), np.random.normal(0, 0.1, m - 1)))  # \"All-but-one\" distribution\n",
    "        #return np.random.normal(mean, loc, m)\n",
    "    \n",
    "def sample_gamma(m):\n",
    "    k, theta = 1, 10\n",
    "    return np.random.gamma(k, theta, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def energy(data, gen, sigma=1., unitize_grads=False, power=1.):\n",
    "    '''Computes abbreviated energy statistic between two point sets.\n",
    "    \n",
    "    The smaller the value, the closer the sets.\n",
    "    Args:\n",
    "      data: 1D numpy array of any length, e.g. 100.\n",
    "      gen: 1D numpy array of any length, e.g. 10.\n",
    "      sigma: Float, kernel lengthscale.\n",
    "      unitize_grads: Boolean, divide gradients by norm.\n",
    "    Returns:\n",
    "      e: Scalar, the energy between the sets.\n",
    "      mmd: Scalar, the mmd between the sets.\n",
    "      gradients_e: Numpy array of energy gradients for each proposal point.\n",
    "      gradients_mmd: Numpy array of mmdgradients for each proposal point.\n",
    "    '''\n",
    "    x = sorted(list(data))\n",
    "    y = sorted(list(gen))\n",
    "    data_num = len(x)\n",
    "    gen_num = len(y)\n",
    "    num_combos_xx = data_num * (data_num - 1) / 2.\n",
    "    num_combos_yy = gen_num * (gen_num - 1) / 2.\n",
    "\n",
    "\n",
    "    # Compute energy.\n",
    "    v = np.concatenate((x, y), 0)\n",
    "    v_vert = v.reshape(-1, 1)\n",
    "    v_tiled = np.tile(v_vert, (1, len(v)))\n",
    "    pairwise_difs = v_tiled - np.transpose(v_tiled)\n",
    "    \n",
    "    #energy = abs(pairwise_difs)\n",
    "    _energy = abs(pairwise_difs)\n",
    "    energy = np.power(_energy, power)  # Raises |x_i - x_j|^p\n",
    "    \n",
    "    energy_xx = energy[:data_num, :data_num]\n",
    "    energy_yy = energy[data_num:, data_num:]\n",
    "    energy_yx = energy[data_num:, :data_num]\n",
    "    e = (2. / gen_num / data_num * np.sum(energy_yx) -\n",
    "         1. / data_num / data_num * np.sum(energy_xx) -\n",
    "         1. / gen_num / gen_num * np.sum(energy_yy))\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute MMD.\n",
    "    pairwise_prods = np.matmul(v_vert, np.transpose(v_vert))\n",
    "    sqs_vert = np.reshape(np.diag(pairwise_prods), [-1, 1])\n",
    "    sqs_vert_tiled_horiz = np.tile(sqs_vert, (1, data_num + gen_num))\n",
    "    exp_object = (sqs_vert_tiled_horiz - 2 * pairwise_prods +\n",
    "                  np.transpose(sqs_vert_tiled_horiz))\n",
    "    \n",
    "    K = np.exp(-0.5 / sigma * exp_object)\n",
    "    K_xx = K[:data_num, :data_num]\n",
    "    K_yy = K[data_num:, data_num:]\n",
    "    K_xy = K[:data_num, data_num:]\n",
    "    K_xx_upper = np.triu(K_xx, 1)\n",
    "    K_yy_upper = np.triu(K_yy, 1)\n",
    "    mmd = (1. / num_combos_xx * np.sum(K_xx_upper) +\n",
    "           1. / num_combos_yy * np.sum(K_yy_upper) -\n",
    "           2. / data_num / gen_num * np.sum(K_xy))\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute energy gradients.\n",
    "    # TODO: CHECK WHETHER THIS GRADIENT IS CORRECT.\n",
    "    signed = np.sign(pairwise_difs)\n",
    "    signed_yx = signed[data_num:, :data_num]\n",
    "    signed_yy = signed[data_num:, data_num:]\n",
    "    gradients_e = []\n",
    "    for i in range(gen_num):\n",
    "        grad_yi = (2. / gen_num / data_num * sum(signed_yx[i]) - \n",
    "                   2. / gen_num / gen_num * sum(signed_yy[i]))\n",
    "        gradients_e.append(grad_yi)\n",
    "    gradients_e[0] *= 2\n",
    "    gradients_e[-1] *= 2\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute MMD gradients.\n",
    "    mmd_grad = K * (-1. / sigma * pairwise_difs)\n",
    "    mmd_grad_yx = mmd_grad[data_num:, :data_num] \n",
    "    mmd_grad_yy = mmd_grad[data_num:, data_num:] \n",
    "    mmd_grad_yy_upper = np.triu(mmd_grad_yy, 1)\n",
    "    gradients_mmd = []\n",
    "    for i in range(gen_num):\n",
    "        grad_yi = (1. / num_combos_yy * \n",
    "                       (sum(mmd_grad_yy_upper[i]) -\n",
    "                        sum(mmd_grad_yy_upper[:, i])) -\n",
    "                   2. / gen_num / data_num * sum(mmd_grad_yx[i]))\n",
    "                   \n",
    "        gradients_mmd.append(grad_yi)\n",
    "    \"\"\"\n",
    "    if unitize_grads:\n",
    "        gradients_e /= np.linalg.norm(gradients_e) + 1e-6\n",
    "\n",
    "    return e, np.array(gradients_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(data, gen, max_iter=500, learning_rate=1e-2):\n",
    "    '''Runs alternating optimizations, n times through proposal points.\n",
    "    Args:\n",
    "      data: 1D numpy array of any length, e.g. 100.\n",
    "      gen: 1D numpy array of any length, e.g. 10.\n",
    "      max_iter: Scalar, number of times to loop through updates for all vars.\n",
    "      learning_rate: Scalar, amount to move point with each gradient update.\n",
    "      \n",
    "    Returns:\n",
    "      y_out: 2D numpy array of trace of generated proposal points.\n",
    "      e_out: Float, energy between data and last iteration of y_out.\n",
    "    '''\n",
    "    stability_count = 0\n",
    "    stability_period = 50\n",
    "    stability_stopping_criterion = 1e-5\n",
    "\n",
    "    y_out = np.zeros((max_iter, len(gen)))\n",
    "    max_grad = np.zeros(max_iter)\n",
    "    \n",
    "    # Run optimization steps.\n",
    "    for it in range(max_iter):\n",
    "        e_out, e_grads = energy(data, gen)\n",
    "        gen -= learning_rate * e_grads\n",
    "        y_out[it, :] = gen\n",
    "        max_grad[it] = np.max(np.abs(e_grads))\n",
    "        \n",
    "        # Early stopping criterion.\n",
    "        #if it > stability_period and it % stability_period == 0:\n",
    "        if it in [2, 4, 6, 8, 10, 1000]:\n",
    "            # Measure stability.\n",
    "            start_index = np.max((0, it - stability_period))            \n",
    "            moving_var_max_grad = np.var(max_grad[start_index:it])\n",
    "            #moving_var = np.max(np.var(y_out[start_index:it, :], axis=0))\n",
    "            \n",
    "            #if moving_var < stability_stopping_criterion:\n",
    "            #    break\n",
    "            \n",
    "            #print('it: {}, grads: {}, moving_var_max_grad: {:.3f}'.format(\n",
    "            #    it,\n",
    "            #    np.sort(np.abs(e_grads))[-5:],\n",
    "            #    moving_var_max_grad))\n",
    "\n",
    "    return y_out[:it], e_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_single_run(y_out, e_out, p, num_support,\n",
    "                    percentiles=None):\n",
    "    data_markers_x = [y_out.shape[0]] * len(p)\n",
    "    data_markers_y = p\n",
    "    plt.scatter(data_markers_x, data_markers_y, marker='x', label='data')\n",
    "    plt.plot(y_out, 'k-', alpha=0.3)\n",
    "    if percentiles is not None:\n",
    "        plt.scatter([1.05 * y_out.shape[0]] * y_out.shape[1],\n",
    "                    percentiles, color='blue', label='pct')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title('|data|=m={}, |support|=n={}, e(x,y)={:.4f}'.format(\n",
    "              len(p), num_support, e_out))\n",
    "    plt.savefig('support_points_path.png');\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contained_within(a, b, eps=1e-1):\n",
    "    \"\"\"Checks if a is contained within b.\n",
    "       Arguments are 1d arrays. \n",
    "    \"\"\"\n",
    "    condition1 = np.min(a) >= np.min(b) - eps\n",
    "    condition2 = np.max(a) <= np.max(b) + eps\n",
    "    valid = condition1 and condition2\n",
    "    if not condition1:\n",
    "        print('condition1: not {} >= {}'.format(np.min(a), np.min(b) - eps))\n",
    "    if not condition2:\n",
    "        print('condition2: not {} <= {}'.format(np.max(a), np.max(b) + eps))\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_support_points(data, num_support, max_iter, lr, percentiles, plot=False):\n",
    "    # Initialize generated particles for both sets (y and y_).\n",
    "    x = data\n",
    "    x_range = max(x) - min(x)\n",
    "    y_orig_offset = x_range * 0.01\n",
    "    y = list(np.linspace(min(x) + y_orig_offset,\n",
    "                         max(x) - y_orig_offset,\n",
    "                         num_support))\n",
    "\n",
    "    \"\"\"\n",
    "    all_but_one = 1\n",
    "    if all_but_one:\n",
    "        x_range = max(x[1:]) - min(x[1:])\n",
    "        y_orig_offset = x_range * 0.01\n",
    "        y_no10 = list(np.linspace(min(x[1:]) + y_orig_offset,\n",
    "                             max(x[1:]) - y_orig_offset,\n",
    "                             num_support - 1))\n",
    "        y = np.concatenate(([10], y_no10))\n",
    "    \"\"\" \n",
    "    #y = percentiles\n",
    "   \n",
    "    # Optimize particles for each dataset (x0 and x1).\n",
    "    y_out, e_out = optimize(x, y, max_iter=max_iter, learning_rate=lr)\n",
    "    lr_updated = lr\n",
    "    max_iter_updated = max_iter\n",
    "    \n",
    "    # ADJUSTMENT CRITERIA.\n",
    "    # desirable behavior.\n",
    "    # if x contains 10:\n",
    "    #   if n < N/2:\n",
    "    #     while y not contained within x: reduce lr.\n",
    "    #     while y not contained within x[1:], increase max_iter.\n",
    "    #   elif n >= N/2:\n",
    "    #     while y not contained within x, reduce lr.\n",
    "    # else:\n",
    "    #   change starting support point set to not include 10\n",
    "    #   while y not contained within x, reduce lr.\n",
    "\n",
    "    \"\"\"\n",
    "    if 10 in x:\n",
    "        if num_support <= len(x) / 2.:  # All support points should be within\n",
    "            while not contained_within(y_out, x):  # within 10\n",
    "                # Reduce lr and try again.\n",
    "                lr_updated *= 0.5\n",
    "                print('Retrying with lr = {}'.format(lr_updated))\n",
    "                y_out, e_out = optimize(x, y, max_iter=max_iter, learning_rate=lr_updated)\n",
    "            print('Satisfied \"within 10\" with lr={}'.format(lr_updated))      \n",
    "                \n",
    "            while not contained_within(y_out[0], x[1:]):  # within core\n",
    "                # Increase max_iter.\n",
    "                max_iter_updated *= 2\n",
    "                y_out, e_out = optimize(x, y, max_iter=max_iter_updated, learning_rate=lr_updated)\n",
    "                \n",
    "    elif (10 in x) and (num_support >= len(x) / 2.):\n",
    "        while not contained_within(y_out[-1], x):\n",
    "            # Reduce lr and try again.\n",
    "            lr_updated *= 0.5\n",
    "            y_out, e_out = optimize(x, y, max_iter=max_iter, learning_rate=lr_updated)\n",
    "            print('Retrying with lr = {}'.format(lr_updated))\n",
    "        \n",
    "    elif 10 not in x:\n",
    "        while not contained_within(y_out[-1], x):  # within core\n",
    "            # Increase max_iter.\n",
    "            max_iter_updated *= 2\n",
    "            y_out, e_out = optimize(x, y, max_iter=max_iter_updated, learning_rate=lr_updated)\n",
    "        \n",
    "    print('RESOLVED within x')\n",
    "    plot_single_run(y_out, e_out, x, num_support)\n",
    "    \"\"\"\n",
    "    ############################\n",
    "\n",
    "\n",
    "\n",
    "    # Get last updated set as support points.\n",
    "    sp = y_out[-1]\n",
    "    \n",
    "    # Plot optimization.\n",
    "    if plot:\n",
    "        plot_single_run(y_out, e_out, x, num_support,\n",
    "                        percentiles=percentiles)\n",
    "        \n",
    "    return sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_eps(discrepancies_hist, discrepancies_cdf, delta):\n",
    "    eps = np.percentile(discrepancies_hist, 100*(1 - delta))\n",
    "    print('\\nPr(d_hist(Y, Y\\') > {:.2f}) < {:.2f}'.format(eps, delta))\n",
    "    eps = np.percentile(discrepancies_cdf, 100*(1 - delta))\n",
    "    print('\\nPr(d(Y, Y\\') > {:.2f}) < {:.2f}'.format(eps, delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_with_baseline(data_source, max_discs_shift_y_y_, max_discs_hist_y_y_,\n",
    "                       max_discs_y_y_, max_discs_x_y_, min_pstats_x_y_, n, m):\n",
    "    \"\"\"Shows how well support points fit true distribution.\n",
    "    \n",
    "    Compares X_n and Y_n' to distribution X.\n",
    "    \n",
    "    Args:\n",
    "      data_source: String, indicates which distribution to sample.\n",
    "      max_discs_shift_y_y_: Array, privacy discrepancies (shift).\n",
    "      max_discs_hist_y_y_: Array, privacy discrepancies (pdf).\n",
    "      max_discs_y_y_: Array, privacy discrepancies (cdf).\n",
    "      max_discs_x_y_: Array, utility discrepancies.\n",
    "      min_pstats_x_y_: Array, utility KS p-statistics.\n",
    "      m: Int, number of points from reference distribution.\n",
    "      n: Int, number of points from testing distribution.\n",
    "    \"\"\"\n",
    "    if data_source == 'gaussian':\n",
    "        sample = sample_gaussian\n",
    "    elif data_source == 'gamma':\n",
    "        sample = sample_gamma\n",
    "    \n",
    "    # Get discs for X (n samples) and Xm (m samples).\n",
    "    num_trials = len(max_discs_x_y_)\n",
    "    discs_x_xn, pstats_x_xn = (\n",
    "        zip(*(ks_2samp(sample(m), sample(n)) for _ in range(num_trials))))\n",
    "    \n",
    "    # Compare discrepancies.\n",
    "    plt.hist([max_discs_y_y_, max_discs_x_y_, discs_x_xn], histtype='bar',\n",
    "             bins=20, label=['d(Y, Y\\')', 'd(X, Y\\')', 'd(X, X_n)'],\n",
    "             color=['green', 'blue', 'gray'], alpha=0.3)\n",
    "    plt.title('Baseline comparison: KS distances')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare p-statistics of Y_n' and X_n, each to X.\n",
    "    plt.hist([pstats_x_xn, min_pstats_x_y_], bins=20,\n",
    "             label=['Xn diff from X?', 'Y\\' diff from X?'],\n",
    "             color=['blue', 'green'], alpha=0.3)\n",
    "    plt.title('Baseline comparison: KS p-stats')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    tail = 5\n",
    "    print('max_discs_shift_y_y_: {}'.format(np.round(sorted(max_discs_shift_y_y_), 3)[-tail:]))\n",
    "    print('max_discs_hist_y_y_: {}'.format(np.round(sorted(max_discs_hist_y_y_), 3)[-tail:]))\n",
    "    print('max_discs_y_y_: {}'.format(np.round(sorted(max_discs_y_y_), 3)[-tail:]))\n",
    "    print('max_discs_x_y_: {}'.format(np.round(sorted(max_discs_x_y_), 3)[-tail:]))\n",
    "    print('max_discs_x_xn_: {}'.format(np.round(sorted(discs_x_xn), 3)[-tail:]))\n",
    "    \n",
    "    print('min_pstats_x_xn: {}'.format(np.round(sorted(pstats_x_xn), 3)[:tail]))\n",
    "    print('min_pstats_x_y_: {}'.format(np.round(sorted(min_pstats_x_y_), 3)[:tail]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def middle_n_percentiles(x, n): \n",
    "    return np.percentile(x, np.linspace(1./(n+1), 1 - 1./(n+1), n) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_trial(data_source, m, n, max_iter, lr, plot=False, single_run=False):\n",
    "    \"\"\"Finds support point discrepancies due to single-point omission.\n",
    "    \n",
    "    Args:\n",
    "      data_source: String, identifier for dataset to use.\n",
    "      m: Int, number of data points in X. So X' has n-1.\n",
    "      n: Int, number of support points in Y and Y'.\n",
    "      max_iter: Number of iterations in support point optimization.\n",
    "      lr: Float, learning rate for energy optimization.\n",
    "    \"\"\"\n",
    "    # Collect outputs for Y and Y', and for X and Y'.\n",
    "    discs_shift_y_y_ = []\n",
    "    discs_hist_y_y_ = []\n",
    "    discs_y_y_ = []\n",
    "    discs_x_y_ = []\n",
    "    pstats_x_y_ = []\n",
    "    \n",
    "    # Sample n from reference distribution.\n",
    "    if data_source == 'gaussian':\n",
    "        x = sample_gaussian(m)\n",
    "    elif data_source == 'gamma':\n",
    "        x = sample_gamma(m)\n",
    "    else:\n",
    "        print('Set up fn for unknown data_source')\n",
    "        sys.exit()\n",
    "\n",
    "    # Compute m support points on full set.\n",
    "    percentiles = middle_n_percentiles(x, n)\n",
    "\n",
    "    y = get_support_points(x, n, max_iter, lr, percentiles, plot=plot)\n",
    "    #y = percentiles\n",
    "\n",
    "    dist_maxSP_n1data = max(y) - max(x[1:])\n",
    "    \n",
    "    # Compare support points on full set vs single-point omission set.\n",
    "    #for removed_index in range(m):\n",
    "    for removed_index in [0]:\n",
    "\n",
    "        # Remove the current index to make the \"neighboring\" set.\n",
    "        x_ = [v for i,v in enumerate(x) if i != removed_index]\n",
    "        #x_ = [v if i != removed_index else sample_gaussian(1)[0] for i,v in enumerate(x)]\n",
    "\n",
    "        # Compute reduced set representation, i.e. support points or percentiles.\n",
    "        percentiles = middle_n_percentiles(x_, n)\n",
    "\n",
    "        y_ = get_support_points(x_, n, max_iter, lr, percentiles, plot=plot)\n",
    "        #y_ = percentiles_\n",
    "        \n",
    "        \n",
    "        # ----------------------\n",
    "        # Compute Discrepancies.\n",
    "        # ----------------------\n",
    "        # Get distance between full-set and single-point omission support points.\n",
    "        disc_y_y_, _ = ks_2samp(y, y_)\n",
    "        disc_x_y_, pstat_x_y_ = ks_2samp(x, y_)\n",
    "        \n",
    "        # Get maximal difference in histogram density between y and y_.\n",
    "        min_y = np.min(np.concatenate((y, y_)))\n",
    "        max_y = np.max(np.concatenate((y, y_)))\n",
    "        h_y = np.histogram(y, bins=np.linspace(min_y, max_y, n), density=True)\n",
    "        h_y_ = np.histogram(y_, bins=np.linspace(min_y, max_y, n), density=True)\n",
    "        disc_hist_y_y_ = np.max(np.abs(h_y[0] - h_y_[0]))\n",
    "        \n",
    "        # Get maximal shift in support points.\n",
    "        disc_shift_y_y_ = np.max(np.abs(np.sort(y) - np.sort(y_)))\n",
    "        \n",
    "        # ----------------------\n",
    "        \n",
    "        \n",
    "        # Add results to list for this sample x.\n",
    "        discs_shift_y_y_.append(disc_shift_y_y_)\n",
    "        discs_hist_y_y_.append(disc_hist_y_y_)\n",
    "        discs_y_y_.append(disc_y_y_)\n",
    "        discs_x_y_.append(disc_x_y_)\n",
    "        pstats_x_y_.append(pstat_x_y_)\n",
    "        \n",
    "        plot_verbose = 0\n",
    "        if plot_verbose:\n",
    "            # Hist of PDFs.\n",
    "            plt.hist([y, y_], bins=20, label=['y', 'y_'], density=True)\n",
    "            plt.title('PDFs max_density_dif(Ym, Ym_) = {:.3f}'.format(disc_hist_y_y_))\n",
    "            plt.show()\n",
    "            \n",
    "            # eCDF of Y and Y'.\n",
    "            plt.scatter(x[removed_index], 0.0, label='pt removed')\n",
    "            plt.step(np.sort(y), np.arange(1, len(y) + 1) / float(len(y)),\n",
    "                     label='y', color='blue', alpha=0.7, where='post')\n",
    "            plt.step(np.sort(y_), np.arange(1, len(y_) + 1) / float(len(y_)),\n",
    "                     label='y_', color='green', alpha=0.7, where='post')\n",
    "            plt.legend()\n",
    "            plt.ylim((-0.1, 1.1))\n",
    "            plt.title('CDFs. KS(Ym, Ym_) = {:.3f}'.format(disc_y_y_))\n",
    "            plt.show()\n",
    "            \n",
    "            # eCDF of percentiles and percentiles_.\n",
    "            plt.scatter(x[removed_index], 0.0, label='pt removed')\n",
    "            plt.step(percentiles, np.arange(1, len(percentiles) + 1) / float(len(percentiles)),\n",
    "                     label='pct', color='blue', alpha=0.7, where='post')\n",
    "            plt.step(percentiles_, np.arange(1, len(percentiles_) + 1) / float(len(percentiles_)),\n",
    "                     label='pct_', color='green', alpha=0.7, where='post')\n",
    "            plt.legend()\n",
    "            plt.ylim((-0.1, 1.1))\n",
    "            plt.title('CDFs. Percentiles.'.format(disc_y_y_))\n",
    "            plt.show()\n",
    "            \n",
    "            # eCDF of X and Y'.\n",
    "            plt.scatter(x[removed_index], 0.0, label='pt removed')\n",
    "            plt.step(np.sort(x), np.arange(1, len(x) + 1) / float(len(x)),\n",
    "                     label='x', color='gray', alpha=0.5, where='post')\n",
    "            plt.step(np.sort(y_), np.arange(1, len(y_) + 1) / float(len(y_)),\n",
    "                     label='y_', color='green', alpha=0.7, where='post')\n",
    "            plt.legend()\n",
    "            plt.ylim((-0.1, 1.1))\n",
    "            plt.title('CDFs. KS(Xn, Ym_) = {:.3f}'.format(disc_x_y_))\n",
    "            plt.show()\n",
    "\n",
    "            print('x: {}'.format(np.round(sorted(x), 2)))\n",
    "            print('y: {}'.format(np.round(sorted(y), 2)))\n",
    "            print('y_: {}'.format(np.round(sorted(y_), 2)))\n",
    "\n",
    "        if single_run:\n",
    "            break\n",
    "\n",
    "    # Compute max shift, add associated noise to support points,\n",
    "    # sample from kernels centered at noised support points,\n",
    "    # measure accuracy (x, y), measure accuracy (x, g(y^))\n",
    "    max_shift = np.max(discs_shift_y_y_)\n",
    "    y_noisy = [i + np.random.laplace(max_shift) for i in y]\n",
    "    x_noisy = [np.random.laplace(np.random.choice(y), max_shift) for _ in x]\n",
    "    disc_x_x_noisy, _ = energy(x, x_noisy)\n",
    "    disc_x_y_noisy, _ = energy(x, y_noisy)\n",
    "    disc_x_y, _ = energy(x, y)\n",
    "    if plot:\n",
    "        plt.hist(x, bins=20, alpha=0.3, label='x', density=True)\n",
    "        plt.hist(x_noisy, bins=20, alpha=0.3, label='x_noisy', density=True)\n",
    "        plt.title('disc_x_x_noisy: {:.3f}'.format(disc_x_x_noisy))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    return (dist_maxSP_n1data, discs_shift_y_y_, discs_hist_y_y_, discs_y_y_, \n",
    "            discs_x_y_, pstats_x_y_, disc_x_x_noisy, disc_x_y_noisy, disc_x_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run support point vs. LOO support point experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_trials_mn(m, n, num_trials, lr, max_iter):\n",
    "    delta = 1. / m  # Probability of exposing a single point.\n",
    "    data_source = 'gaussian'\n",
    "\n",
    "    # Run trials.\n",
    "    dists_maxSP_n1data = []\n",
    "    max_discs_shift_y_y_ = []\n",
    "    max_discs_hist_y_y_ = []\n",
    "    max_discs_y_y_ = []\n",
    "    max_discs_x_y_ = []\n",
    "    min_pstats_x_y_ = []\n",
    "    discs_x_x_noisy = []\n",
    "    discs_x_y_noisy = []\n",
    "    discs_x_y = []\n",
    "\n",
    "    start_time =  time.time()\n",
    "    for i in range(num_trials):\n",
    "        if i % 10 == 0:\n",
    "            print('Run {}'.format(i))\n",
    "    \n",
    "        # Get discrepancies for privacy and utility pairs.\n",
    "        plot = True if i == 0 else False  # Plot only on first exp.\n",
    "        #plot = True\n",
    "\n",
    "        # Runs trial for one sample X, and all LOO Y's.\n",
    "        (dist_maxSP_n1data,\n",
    "         discs_shift_y_y_,\n",
    "         discs_hist_y_y_,\n",
    "         discs_y_y_,\n",
    "         discs_x_y_,\n",
    "         pstats_x_y_,\n",
    "         disc_x_x_noisy,\n",
    "         disc_x_y_noisy,\n",
    "         disc_x_y) = run_trial(data_source, m, n, max_iter, lr, plot=plot, single_run=True)\n",
    "\n",
    "        dists_maxSP_n1data.append(dist_maxSP_n1data)\n",
    "        max_discs_shift_y_y_.append(np.max(discs_shift_y_y_))\n",
    "        max_discs_hist_y_y_.append(np.max(discs_hist_y_y_))\n",
    "        max_discs_y_y_.append(np.max(discs_y_y_))\n",
    "        max_discs_x_y_.append(np.max(discs_x_y_))\n",
    "        min_pstats_x_y_.append(np.min(pstats_x_y_))\n",
    "        discs_x_x_noisy.append(disc_x_x_noisy)\n",
    "        discs_x_y_noisy.append(disc_x_y_noisy)\n",
    "        discs_x_y.append(disc_x_y)        \n",
    "    \n",
    "    return max_discs_shift_y_y_, discs_x_x_noisy, dists_maxSP_n1data\n",
    "    \n",
    "    \"\"\"\n",
    "    # Utility: How does Y_m' compare to X_m, in belonging to X? \n",
    "    eval_with_baseline(data_source, max_discs_shift_y_y_, \n",
    "                       max_discs_hist_y_y_, max_discs_y_y_,\n",
    "                       max_discs_x_y_, min_pstats_x_y_, m, n)\n",
    "    \n",
    "    # Privacy: How close is Y_m' to Y_m?\n",
    "    measure_eps(max_discs_hist_y_y_, max_discs_y_y_, delta)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print('Time elapsed: {:.1f}s'.format(end_time - start_time))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DO_EXPERIMENT_supportpoints:\n",
    "    if 0:\n",
    "        m, n = 10, 7\n",
    "        out, _, _ = run_trials_mn(m, n, num_trials)\n",
    "        print('m: {}, n: {}, max_shift: {:.6f}'.format(m, n, out[0]))\n",
    "        pdb.set_trace()\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    # -------------- main() ------------------\n",
    "    #############################################\n",
    "    \n",
    "    m = 10\n",
    "    support_point_grid = range(2, 4, 1)\n",
    "    num_trials = 5  # Num times to test each m-n combination.\n",
    "    max_iter = 1000  # Num iterations in each support point optimization.\n",
    "    lr = 0.1 # Energy optimization learning rate.\n",
    "    \n",
    "    max_shifts_all = np.zeros((len(support_point_grid), num_trials))\n",
    "    discs_x_x_noisy_all = np.zeros((len(support_point_grid), num_trials))\n",
    "    dists_maxSP_n1data_all = np.zeros((len(support_point_grid), num_trials))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, n in enumerate(support_point_grid):\n",
    "        (max_shifts_mn,\n",
    "         discs_x_x_noisy,\n",
    "         dists_maxSP_n1data) = run_trials_mn(m, n, num_trials, lr, max_iter)\n",
    "        \n",
    "        max_shifts_all[i] = max_shifts_mn\n",
    "        discs_x_x_noisy_all[i] = discs_x_x_noisy\n",
    "        dists_maxSP_n1data_all[i] = dists_maxSP_n1data\n",
    "        \n",
    "        print(('m: {}, n: {}, max_shift: {:.3f}, disc_x_x_noisy: {:.3f}, '\n",
    "               'dist_maxSP_n1data: {:.3f}').format(\n",
    "            m, n, np.median(max_shifts_mn), np.median(discs_x_x_noisy),\n",
    "            np.median(dists_maxSP_n1data)))\n",
    "\n",
    "    end_time = time.time()\n",
    "    print('Time elapsed: {:.1f}s'.format(end_time - start_time))\n",
    "    \n",
    "    # Plot results.\n",
    "    plt.plot(support_point_grid, np.median(max_shifts_all, axis=1))\n",
    "    plt.xlabel('num support points')\n",
    "    plt.ylabel('median of max_shifts over trials')\n",
    "    plt.show()\n",
    "    plt.plot(support_point_grid, np.median(discs_x_x_noisy_all, axis=1))\n",
    "    plt.xlabel('num support points')\n",
    "    plt.ylabel('median of disc_x_x_noisy over trials')\n",
    "    plt.show()\n",
    "    plt.plot(support_point_grid, np.median(dists_maxSP_n1data_all, axis=1))\n",
    "    plt.xlabel('num support points')\n",
    "    plt.ylabel('median of dist_maxSP_n1data_all over trials')\n",
    "    plt.show()\n",
    "    \n",
    "    print(np.min(dists_maxSP_n1data_all, axis=1))\n",
    "\n",
    "    pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2:\n",
    "## TensorFlow attack. Identify missing Xi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the training set of datasets and their support points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_nn_training_set(CONFIG):\n",
    "    n = CONFIG['energy_n']\n",
    "    m = CONFIG['energy_m']\n",
    "    lr = CONFIG['energy_lr']\n",
    "    max_iter = CONFIG['energy_max_iter']\n",
    "    data_source = CONFIG['data_source']\n",
    "    data_size = CONFIG['tf_data_size']\n",
    "    \n",
    "    assert data_source == 'gaussian', 'Temporarily only Gaussian(0,1)'\n",
    "    \n",
    "    data = np.zeros((data_size, n + m + 1))  # Add one for removed index.\n",
    "    for i in range(data_size):\n",
    "        x = np.random.normal(0, 1, size=n)\n",
    "        removed_index = np.random.choice(range(n))\n",
    "        x_ = [v for i,v in enumerate(x) if i != removed_index]\n",
    "        percentiles = middle_n_percentiles(x_, n)\n",
    "\n",
    "        y_ = get_support_points(x_, m, max_iter, lr, percentiles)\n",
    "        \n",
    "        # Assemble example with original data, LOO support points,\n",
    "        # and removed index.\n",
    "        data[i] = np.concatenate((np.sort(x), np.sort(y_), [removed_index]))\n",
    "    \n",
    "    dim_x, dim_y = n, m\n",
    "    data_train = data[:int(0.8 * data.shape[0])]\n",
    "    data_test = data[int(0.8 * data.shape[0]):]\n",
    "    \n",
    "    np.save('nn_training_set.npy', data_train)\n",
    "    return data_train, data_test, dim_x, dim_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the TF graph and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_tf(CONFIG, data_train, data_test):\n",
    "    import tensorflow as tf\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    width = CONFIG['tf_width']\n",
    "    depth = CONFIG['tf_depth']\n",
    "    lr = CONFIG['tf_lr']\n",
    "    max_iter = CONFIG['tf_max_iter']\n",
    "    activation = CONFIG['tf_activation']\n",
    "    regularizer = CONFIG['tf_regularizer']\n",
    "\n",
    "    # Set up config for neural network.\n",
    "    data_train_inputs, data_train_labels = data_train[:,:-1], data_train[:,-1]\n",
    "    data_test_inputs, data_test_labels = data_test[:,:-1], data_test[:,-1]\n",
    "    n_inputs = data_train_inputs.shape[0]\n",
    "\n",
    "\n",
    "    ###############\n",
    "    # Build graph.\n",
    "\n",
    "    # Computation.\n",
    "    input_data = tf.placeholder(tf.float32, [None, dim_x + dim_y], name='input_data')\n",
    "    input_labels = tf.placeholder(tf.int32, [None], name='input_labels')\n",
    "    labels = tf.one_hot(input_labels, dim_x)\n",
    "\n",
    "    h = input_data\n",
    "    for d in range(depth):\n",
    "        h = tf.layers.dense(h, width, activation=activation,\n",
    "                            activity_regularizer=regularizer)\n",
    "        h = tf.nn.dropout(h, 0.8)\n",
    "    logits = tf.layers.dense(h, dim_x)\n",
    "\n",
    "    # Loss and optimization.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.losses.softmax_cross_entropy(labels, logits))\n",
    "    opt = tf.train.RMSPropOptimizer(lr)\n",
    "    grads_, vars_ = zip(*opt.compute_gradients(\n",
    "        loss,var_list=tf.trainable_variables()))\n",
    "    grads_clipped_ = tuple([tf.clip_by_value(g, -0.01, 0.01) for g in grads_])\n",
    "    optim = opt.apply_gradients(zip(grads_clipped_, vars_))\n",
    "\n",
    "    # Performance.\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "    correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    #############\n",
    "\n",
    "    # Train.\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    sess_config = tf.ConfigProto(allow_soft_placement=True, gpu_options=gpu_options)\n",
    "    with tf.Session(config=sess_config) as sess:\n",
    "        sess.run(init_op)\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            acc_train_, loss_, _ = sess.run([accuracy, loss, optim], \n",
    "                                {input_data: data_train_inputs,\n",
    "                                 input_labels: data_train_labels})\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                acc_test_ = sess.run(accuracy, \n",
    "                                     {input_data: data_test_inputs,\n",
    "                                      input_labels: data_test_labels})\n",
    "\n",
    "                print(('Iter: {}. , Loss={:.3f}, Acc_train={:.3f}, '\n",
    "                       'Acc_test={:.3f}').format(i, loss_, acc_train_, acc_test_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'energy_m': 50,\n",
    "    'energy_n': 10,\n",
    "    'energy_lr': 1.,\n",
    "    'energy_max_iter': 100,\n",
    "    'data_source': 'gaussian',\n",
    "    'tf_data_size': 250,\n",
    "    'tf_lr': 1e-4,\n",
    "    'tf_max_iter': 500000,\n",
    "    'tf_activation': tf.nn.relu,\n",
    "    'tf_regularizer': None, #tf.contrib.layers.l1_regularizer(0.001),\n",
    "    'tf_width': 100,\n",
    "    'tf_depth': 5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if DO_EXPERIMENT_tensorflow:\n",
    "    print('\\n\\nTrial run for support points. Check for convergence before proceeding.')\n",
    "    run_trial(CONFIG['data_source'], CONFIG['energy_m'], CONFIG['energy_n'],\n",
    "              CONFIG['energy_max_iter'], plot=True)\n",
    "    \n",
    "    print('\\n\\n[Generating training data for TensorFlow prediction model.]')\n",
    "    data_train, data_test, dim_x, dim_y = build_nn_training_set(CONFIG)\n",
    "    \n",
    "    print('\\n\\nTraining TensorFlow prediction model.')\n",
    "    run_tf(CONFIG, data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDF to PDF comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if DO_EXPERIMENT_cdf2pdf:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('ggplot')\n",
    "    scale = 0.\n",
    "\n",
    "    # Histograms.\n",
    "    x = np.random.normal(0, 1, size=20)\n",
    "    y = np.copy(x)\n",
    "    y[np.where(np.abs(x) < 1)] *= scale\n",
    "    plt.hist([x, y], bins=20, alpha=0.3, label=['x', 'y'])\n",
    "    #plt.hist(y, bins=10, alpha=0.3, label='y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # eCDFs\n",
    "    plt.step(np.sort(x), np.arange(1, len(x) + 1) / float(len(x)),\n",
    "             label='cdf', color='red', alpha=0.7, where='post')\n",
    "    plt.step(np.sort(y), np.arange(1, len(y) + 1) / float(len(y)),\n",
    "             label='cdf', color='blue', alpha=0.7, where='post')\n",
    "    plt.legend()\n",
    "    plt.ylim((-0.1, 1.1))\n",
    "    plt.title('CDFs')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical testing for minimum distance between pathological sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=20, m=2, delta*=0.10000, dist*=0.00000\n",
      "n=20, m=3, delta*=0.15000, dist*=-0.00000\n",
      "n=20, m=4, delta*=0.20000, dist*=0.00000\n",
      "n=20, m=5, delta*=0.25000, dist*=0.00000\n",
      "n=20, m=6, delta*=0.30000, dist*=-0.00000\n",
      "n=20, m=7, delta*=0.35000, dist*=-0.00000\n",
      "n=20, m=8, delta*=0.40000, dist*=0.00000\n",
      "n=20, m=9, delta*=0.45000, dist*=-0.00000\n",
      "n=20, m=10, delta*=0.50000, dist*=-0.00000\n",
      "n=20, m=11, delta*=0.55000, dist*=-0.00000\n",
      "n=20, m=12, delta*=0.60000, dist*=0.00000\n",
      "n=20, m=13, delta*=0.65000, dist*=-0.00000\n",
      "n=20, m=14, delta*=0.70000, dist*=0.00000\n",
      "n=20, m=15, delta*=0.75000, dist*=-0.00000\n",
      "n=20, m=16, delta*=0.80000, dist*=0.00000\n",
      "n=20, m=17, delta*=0.85000, dist*=0.00000\n",
      "n=20, m=18, delta*=0.90000, dist*=0.00000\n",
      "n=20, m=19, delta*=0.95000, dist*=0.00000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mmd_utils import compute_mmd\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "if DO_EXPERIMENT_empirical_minima:\n",
    "    \n",
    "    n = 20\n",
    "    x = np.concatenate((np.zeros(n - 1), np.ones(1)))\n",
    "    #x = np.concatenate((np.random.normal(0, 0.1, n - 1), np.ones(1)))\n",
    "    sigma_list = [1.]\n",
    "    power = 2.\n",
    "\n",
    "    m_range = range(2, n)\n",
    "    delta_range = np.arange(-0.5, 1.5, 0.05)\n",
    "    results = np.ones((len(m_range), 4)) * -1.\n",
    "\n",
    "    for i, m in enumerate(m_range):\n",
    "        optimal_delta = -1.\n",
    "        optimal_distance = 1e10\n",
    "        distances = []\n",
    "        for delta in delta_range:\n",
    "            y = np.concatenate((np.zeros(m - 1), np.ones(1) * delta))\n",
    "            #y = np.concatenate((np.random.normal(0, 0.1, m - 1), np.ones(1) * delta))\n",
    "\n",
    "            d_out, _ = energy(x, y, power=power)\n",
    "            #d_out, _ = compute_mmd(x, y, sigma_list=sigma_list)\n",
    "            #d_out = wasserstein_distance(x, y)\n",
    "\n",
    "            distances.append(d_out)\n",
    "            if d_out < optimal_distance:\n",
    "                optimal_delta = delta\n",
    "                optimal_distance = d_out\n",
    "\n",
    "        #plt.plot(delta_range, distances)\n",
    "        #plt.xlabel('delta')\n",
    "        #plt.ylabel('distances')\n",
    "        #plt.title('Distance for range of delta, n={}, m={}'.format(n, m))\n",
    "        #plt.show()\n",
    "\n",
    "        results[i] = [n, m, optimal_delta, optimal_distance]\n",
    "\n",
    "    for row in results:\n",
    "        print('n={:.0f}, m={:.0f}, delta*={:.5f}, dist*={:.5f}'.format(\n",
    "            row[0], row[1], row[2], row[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
